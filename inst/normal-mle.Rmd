---
title: "Normal distribution MLE"
output:
    rmarkdown::html_vignette:
        toc: true
vignette: >
  %\VignetteIndexEntry{Normal distribution MLE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{ggplot2}
  %\VignetteDepends{dplyr}
  %\VignetteDepends{tibble}
  %\VignetteDepends{algebraic.mle}
  %\VignetteDepends{boot}
  %\VignetteKeyword{normal}
  %\VignetteKeyword{maximum likelihood estimation} 
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(digits=3)
options(scipen=999)
```

## Introduction
To demonstrate the R package `algebraic.mle`, we consider the relatively
simple case of a random sample from i.i.d. normally distributed random
variables. First, we load the R package `algebraic.mle` with:
```{r setup, warning=F, message=F}
library(algebraic.mle)
library(algebraic.dist)
library(likelihood.model)
library(boot)
```

## Generating a sample

We define the parameters of the i.i.d. random sample with:
```{r}
n <- 50
theta <- c(4,2)
```

We generate a random sample $X_i \sim \operatorname{N}(\mu=`r theta[1]`,\sigma^2=`r theta[2]`)$ for
$i=1,\ldots,n$ with:
```{r}
x <- rnorm(n,mean=theta[1],sd=sqrt(theta[2]))
```

We have observed a sample of size $n=`r n`$.
We show some observations from this sample (data frame) with:
```{r}
head(x,n=4)
```

We show a histogram of the sample with:
```{r}
hist(x)
```

We see the characteristic bell shaped curve of the normal distribution.
If we did not a prior know that the data was normally distributed, this would
be evidence that the normal distribution is a good fit to the data.

## Maximum likelihood estimation
If we would like to estimate $\theta=(`r theta`)'$, we can do so using maximum likelihood
estimation as implemented by the `algebraic.mle` package:
```{r}
theta.hat <- mle_normal(x)
summary(theta.hat)
```

We can show the point estimate given data `x` with:
```{r}
params(theta.hat)
```

We can show the Fisher information matrix (FIM) and variance-covariance matrix
with:
```{r}
observed_fim(theta.hat)
vcov(theta.hat)
```

We can show the confidence intervals with:
```{r}
confint(theta.hat)
```

A nice property of MLEs is that, asymptotically, they converge to
a normal distribution with a mean given by the true parameter, in this case
$\theta = (\mu,\sigma^2)'$, and a variance-covariance given by the inverse of
the Fisher information matrix evaluated at $\theta$.
This is how we estimated the confidence intervals and other statistics above.

### Performance measures of the MLE
We do not know $\theta$, but we may estimate it from a sample, and thus
we may approximate the sampling distribution of $\hat\theta$ with
$\mathcal{N}(\hat\theta,I^{-1}(\hat\theta))$.

Let $F$ denote the true distribution function such that $X_j \sim F$ for all
$j$.
Suppose we have some population parameter $\theta = t(F)$
and an estimator of $\theta$ given by $\hat\theta = s(\{X_1,\ldots,X_n\})$.
A reasonable requirement for an estimator $\hat\theta$ is that it converges to
the true parameter value $\theta$ as we collect more and more data.
In particular, we say that it is a consistent estimator of $\theta$ if
$\hat\theta$ converges in probability to $\theta$, denoted by
$\hat\theta \overset{p}{\mapsto} \theta$.

If the regularity conditions hold for the MLE, then $\hat\theta$ is
a consistent estimator of $\theta$. However, for finite sample sizes, the
estimator may be biased.
The bias of $\hat\theta$ with respect to $\theta$ is defined as
$$
    \operatorname{bias}(\hat\theta,\theta) = E(\hat\theta) - \theta,
$$
where $\operatorname{bias}(\hat\theta,\theta) = 0$ indicates that $\hat\theta$
is an *unbiased* estimator of $\theta$.

As a function of the true distribution $F$, the bias is unknown and is not a
statistic.
However, in the case of the normal, $\hat\mu$ is unbiased and, analytically, the
bias of $\hat\sigma^2$ is given by $-\frac{1}{n} \sigma^2$:
```{r}
bias(theta.hat,theta)
```
If $\sigma^2$ is not known, we may estimate it by using replacing $\hat\sigma^2$
instead:
```{r}
bias(theta.hat)
```

The mean squared error (MSE) is another performance measure of an estimator.
It is given by
$$
    \operatorname{mse}(\hat\theta) = E\bigl\{(\hat\theta - \theta)^T(\hat\theta - \theta)\bigr\},
$$
Another way to compute the MSE is given by
$$
    \operatorname{mse}(\hat\theta) =
        \operatorname{trace}(\operatorname{cov}(\hat\theta) +
        \operatorname{bias}(\hat\theta)^T
        \operatorname{bias}(\hat\theta).
$$

Here's R code to compute the MSE of $\hat\theta$:
```{r}
mse(theta.hat)         # estimate of MSE
mse(theta.hat, theta)  # true MSE
```
This looks to be a pretty good estimate of the true MSE.

## Invariance property of the MLE

An interesting property of an MLE $\hat\theta$ is that the MLE of $f(\theta)$
is given by $f(\hat\theta)$. What is the distribution of $f(\hat\theta)$?
Asymptotically, it is normally distributed with a mean given by $f(\theta)$ and
a variace-covariance given by the covariance of the sampling distribution of
$f(\hat\theta)$.
We provide two methods to compute the variance-covariance.

### Delta method
If $f$ is differentiable, the variance-covariance is given by
$$
\operatorname{var}(f(\hat\theta)) = \operatorname{E}\bigl\{
    \bigl(f(\hat\theta) - f(\theta)\bigr)^2\bigr\} =
    \operatorname{E}\bigl\{J_f(\hat\theta) I(\hat\theta)^{-1} J_f(\hat\theta)^T\bigr\}.
$$
Here, $J_f(\hat\theta)$ is the Jacobian of $f$ evaluated at $\hat\theta$.

### Monte-carlo method
The delta method requires that $f$ be differentiable, but we may use the
Monte-carlo method to estimate the distribution of $f(\hat\theta)$ for any
function $f$.
We simply sample from the MLE of $\hat\theta$ and apply $f$ to its point
estimates and take the covariance of the sample.

Next, we show how to compute the sampling distribution of $g(\hat\theta)$
for some function $g$ and some MLE $\hat\theta$ using both the delta and mc
methods.

### Example 1
Let $g(\theta) = A \theta + b$ for some matrix $A$ and vector $b$. (This is a
simple linear transformation of $\theta$.) We can define $g$ in R with:
```{r}
A <- matrix(c(2,3),nrow=2)
b <- c(1,0)
g <- function(theta) A %*% theta + b
```

We compute the MLE of $g(\theta)$ with:
```{r}
g.mc <- rmap(theta.hat,g,n=100000L)
g.delta <- rmap(theta.hat,g,method="delta")

round(vcov(g.mc),digits=3)
round(vcov(g.delta),digits=3)
```

They are pretty close.

## Weighted MLE: a weighted sum of maximum likelihood estimators

Since the variance-covariance of an MLE is inversely proportional to the
Fisher information that the MLE is defined with respect to, we can combine
multiple MLEs of $\theta$, each of which may be defined with respect to a
different kind of sample, to arrive at the MLE that incorporates the Fisher
information in all of those samples.

Consider $k$ mutually independent MLEs of parameter $\theta$,
$\hat\theta_1,\ldots,\hat\theta_k$, where $\hat\theta_j \sim N(\theta,I_j^{-1}(\theta))$.
Then, the sampling MLE of $\theta$ that incorporates all of the data in
$\hat\theta_1,\ldots,\hat\theta_k$ is given by the inverse-variance
weighted mean,
$$
    \hat\theta_w = \left(\sum_{j=1}^k I_j(\theta)\right)^{-1} \left(\sum_{j=1}^k I_j(\theta) \hat\theta_j\right),
$$
which, asymptotically, has an expected value of $\theta$ and a variance-covariance
of $\left(\sum_{j=1}^k I_j(\theta)\right)^{-1}$.

### Example 2
To evaluate the performance of the weighted MLE, we generate a sample of
$N=1000$ observations from $\mathcal{N}(\theta)$ and compute the MLE
for the observed sample, denoted by $\hat\theta$.

We then divide the observed sample into $r=5$ sub-samples, each of size
$N/r=100$, and compute the MLE for each sub-sampled, denoted by
$\theta^{(1)},\ldots,\theta^{(r)}$.

Finally, we do a weighted combination these MLEs to form the weighted MLE,
denoted by $\theta_w$:
```{r}
N <- 100
r <- 5
samp <- rnorm(N, mean = theta[1], sd = sqrt(theta[2]))
samp.sub <- matrix(samp, nrow = r)
mles.sub <- list(length = r)
for (i in 1:r)
    mles.sub[[i]] <- mle_normal(samp.sub[i,])

mle.wt <- mle_weighted(mles.sub)
mle <- mle_normal(samp)
```

We show the results in the following R code.
First, we show the weighted MLE and its MSE:
```{r}
#params(mle.wt)
#mse(mle.wt)
```
The MLE for the total sample and its MSE is:
```{r}
params(mle)
mse(mle)
```

We see that $\hat\theta$ and $\hat\theta_w$ model approximately the same sampling
distribution when estimating $\theta$ given i.i.d. samples.

## Bootstrapping the MLEs
Let's compare the earlier results that relied on the large sampling assumption
with the bootstrapped MLE using `mle_boot`.
First, `mle_boot` is just a wrapper for `boot` objects or 
objects like `boot`. Thus to use `mle_boot`, we first need
to call `boot` to bootstrap our MLE for the normal.


one with `mle_normal`: we just need to wrap it in a function that
takes the data as input and returns the MLE of the parameters and then
pass it to `mle_boot` constructor:
```{r}
theta.boot <- mle_boot(
    boot(data = x,
         statistic = function(x, i) point(mle_normal(x[i])),
         R = 1000))
```

We already printed out the `theta.boot` object, which provided a lot of
information about it, but we can obtain specified  statistics from the Bootstrap
MLE using the standard interface in `algorithmic.mle`, e.g.:
```{r}
confint(theta.boot) # confidence interval
(point(theta.boot)) # point estimate
(vcov(theta.boot)) # variance-covariance matrix
(se(theta.boot)) # standard error
(bias(theta.boot)) # bias
(mse(theta.boot)) # mean squared error
```

We see that, for the most part, the results are similar to those obtained
using the large sampling assumption.

## Prediction intervals

Frequently, we are actually interested in predicting the outcome of the
random variable (or vector) that we are estimating the parameters of.

We observed a sample $\mathcal{D} = \{T_i\}_{i=1}^n$ where $T_i \sim N(\mu,\sigma^2)$,
$\theta = (\mu,\sigma^2)^T$ is not known. We compute the MLE of $\theta$,
which, asymptotically, is normally distributed with a mean $\theta$ and a
variance-covariance $I^{-1}(\theta)/n$.

We wish to model the uncertainty of a new observation, $\hat{T}_{n+1}|\mathcal{D}$.
We do so by considering both the uncertainty inherent to the Normal distribution and
the uncertainty of our estimate $\hat\theta$ of $\theta$.
In particular, we let $\hat{T}_{n+1}|\hat\theta \sim N(\hat\mu,\hat\sigma^2)$
and $\hat\theta \sim N(\theta,I^{-1}(\theta)/n)$ (the sampling distribution of the
MLE).
Then, the joint distribution of $\hat{T}_{n+1}$ and $\hat\theta$ has the pdf
given by
$$
    f(t,\theta) = f_{\hat{T}|\hat\theta}(t|\theta=(\mu,\sigma^2)) f_{\hat\theta}(\theta),
$$
and thus to find $f(t)$, we marginalize over $\theta$, obtaining
$$
    f(t) = \int_{-\infty}^\infty \int_{-\infty}^{\infty} f_{\hat{T}_{n+1},\hat\mu,\hat\sigma^2}(t,\mu,\sigma^2) d\mu d\sigma^2.
$$

Given the information in the sample, the uncertainty in the new observation
is characterized by the distribution
$$
    \hat{T}_{n+1} \sim f(t).
$$

It has greater variance than $T_{n+1}|\hat\theta$ because, as stated earlier, we do
not know $\theta$, we only have an uncertain estimate $\hat\theta$.

In `pred`, we compute the predictive interval (PI) of the
distribution of $\hat{T}_{n+1}$ using Monte Carlo simulation, where we replace
the integral with a sum over a large number of draws from the joint distribution
of $\hat{T}_{n+1}$ and $\hat\theta$ and then compute the empirical quantiles.

The function `pred` takes as arguments `x`, in this case an `mle` object, 
and a sampler for the distribution of the random variable of interest, in this
case `rnorm` (the sampler for the normal distribution). The sampler
must be compatible with the output of `point(x)`, whether that output be
a scalar or a vector.
Here is how we compute the PI for $\hat{T}_{n+1}$:
```{r}
pred(x=theta.hat, samp=function(n=1,theta) rnorm(n,theta[1],theta[2]))
```

In general, it will return a $p$-by-$3$ matrix, where $p$ is the dimension of
$T$ and the columns are the mean, lower quantile, and upper quantile of the
predictive distribution.

How does this compare to $T_{n+1}|\hat\theta$? We can compute the 95% quantile
interval for $T_{n+1}|\hat\theta$ using the `qnorm` function:
```{r}
mu <- point(theta.hat)[1]
sd <- sqrt(point(theta.hat)[2])
c(mean=mu,lower=qnorm(.025,mean=mu, sd=sd),upper=qnorm(.975,mean=mu, sd=sd))
```

We see that the 95% quantile interval for $T_{n+1}|\hat\theta$ is smaller
than $\hat{T}_{n+1}$, which is what we expected. After all, there is
uncertainty about the parameter value $\theta$.



## Conclusion
In this vignette, we demonstrated how to use the `algebraic.mle` package to
estimate the sampling distribution of the MLE using the large sampling assumption
and the Bootstrap method. The package provides various functions for obtaining
statistics of the MLE, allowing for a deeper understanding of the properties of
your estimator.