---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# R package: `algebraic.mle`

<!-- badges: start -->
<!-- badges: end -->

An algebra over maximum likelihood estimators (MLE).

MLEs have many desirable, well-defined statistical properties.
We define an algebra over MLEs.

## Installation

You can install the development version of `algebraic.mle` from [GitHub](https://github.com/) with:

```{r}
# install.packages("devtools")
devtools::install_github("queelius/algebraic.mle")
```

## Example: MLE of rate parameter in exponential distribution

Suppose we have a sample of $n=50$ draws from $\operatorname{EXP}(\lambda=1)$.
```{r}
library(stats)
n = 2000
rate = 1
x <- stats::rexp(n,rate)
head(x)
```

Then, we can estimate $\lambda$ with:
```{r}
library(algebraic.mle)
(rate.hat <- mle_exp(x))
```

We can show the point estimator with:
```{r}
point(rate.hat)
```

We can show the variance with:
```{r}
vcov(rate.hat)
```

If `rate.hat` had been a vector, `vcov` would have output a variance-covariance
martix.

We can sample from $\hat\lambda$ with:
```{r}
library(ggplot2)
library(tidyverse)
library(gt)

#tibble(algebraic.mle::distr(rate.hat
#test


rate.sampler <- sampler(rate.hat)
data <- tibble(rate.hat=rate.sampler(100000))
knitr::kable(head(data))

ggplot(data, aes(x=rate.hat)) + geom_histogram()

test <- algebraic.mle::distr(rate.hat,function(x) { x^2 })


#tibble(x=data) %>% ggplot(aes(x=x))

#df <- tibble(x=rrate.mle(2000))
#df
#exp2 <- mle_exp(df$x)
#print(exp2)
```

## Sum of maximum likelihood estimators

Since the variance-covariance of an MLE is inversely proportional to the
Fisher information that the MLE is defined with respect to, we can combine
multiple MLEs of $\theta$, each of which may be defined with respect to a
different kind of sample, to arrive at the MLE that incorporates the Fisher
information in all of those samples.

Consider $k$ mutually independent MLE estimators of parameter $\theta$,
$\hat\theta_1,\ldots,\hat\theta_k$, where $\hat\theta_j \sim N(\theta,I_j^{-1}(\theta))$.

Then, the maximum likelihood estimator of $\theta$ that incorporates all of
the data in $\hat\theta_1,\ldots,\hat\theta_k$ is given by the inverse-variance
weighted mean,
$$
    \hat\theta = \left(\sum I_j(\theta)\right)^{-1} \left(\sum I_j(\theta) \theta_j\right).
$$
