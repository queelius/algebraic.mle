---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# R package: `algebraic.mle`

<!-- badges: start -->
<!-- badges: end -->

An algebra over maximum likelihood estimators (MLE).

MLEs have many desirable, well-defined statistical properties.
We define an algebra over MLEs.

## Installation

You can install the development version of `algebraic.mle` from [GitHub](https://github.com/) with:

```{r,eval=F}
# install.packages("devtools")
devtools::install_github("queelius/algebraic.mle")
```

## Example: MLE of rate parameter in exponential distribution

Consider an exponentially distributed random variable
$X_i \sim \operatorname{EXP}(\lambda=1)$ and we draw a random sample from it:

```{r}
library(stats)
library(tidyverse)
n <- 1000
rate <- 1
x <- tibble(x=stats::rexp(n,rate))
```

We have observed a sample of size $n=`r n`$.
We show some observations from this sample (data frame) with:
```{r}
print(x)
```

We show a histogram of the sample, and a plot of the exponential function's
pdf, with:
```{r}
library(ggplot2)
ggplot(x, aes(x=x)) + geom_histogram(aes(y=..density..),alpha=.2) +
    xlim(0,6) + 
    geom_function(fun=dexp)
```

If we would like to estimate $\lambda$, we can do so using maximum likelihood
estimation as implemented by the `algebraic.mle` package:
```{r}
library(algebraic.mle)
(rate.hat <- mle_exp(x$x))
```

We can show the point estimator with:
```{r}
point(rate.hat)
```

We can show the Fisher information and variance-covariance matrices with:
```{r}
fisher_info(rate.hat)
vcov(rate.hat)
```

(If `rate.hat` had been a vector, `vcov` would have output a variance-covariance
matrix. We may consider the above outputs $1 \times 1$ matrices.)

We can show the confidence interval with:
```{r}
confint(rate.hat)
```

### Sampling distribution of the MLE
In general, to estimate the sampling distribution, we generate $B=10000$ samples
(of size $`r n`$) and their corresponding estimators,
$\hat\theta^{(1)},\ldots,\hat\theta^{(B)}$.
We observe the empirical sampling distribution of $\hat\theta$ with:
```{r}
B <- 1000
data0 <- numeric(length=B)
for (i in 1:B)
{
    x <- stats::rexp(n,rate)
    data0[i] <- point(mle_exp(x))
}
ggplot(tibble(rate.hat=data0), aes(x=rate.hat)) +
    geom_histogram(alpha=.2,bins=50) +
    xlim(.89,1.11) + 
    geom_function(fun=rnorm,sigma=sqrt(n/rate))

```

We overlay the theoretical normal density plot.

Normally, we do not have $B$ samples, and if we did, we would incorporate
all $B$ samples into one sample, since the larger sample would contain more
Fisher information about $\theta$.

However, a nice property of MLEs is that, asymptotically, they converge to
a normal distribution with a mean given by the true parameter and a
variance-covariance given by the inverse of the Fisher information matrix, i.e.,
$\hat\lambda \sim N(\lambda,I^{-1}(\lambda))$ where $I$ is the Fisher
information matrix.
In this case, it is $I(\lambda) = n/\lambda^2$.
We do not know $\lambda$, but we may estimate it from a sample, and thus
we may approximate the sampling distribution of $\hat\lambda$ with
$N(\hat\lambda,I^{-1}(\hat\lambda))$.

We sample from the asymptotic distribution of $\hat\lambda$ with:
```{r}
data1 <- sampler(rate.hat)(B)
```

Now, we wish to compare the empirical sampling distribution, `data0`, with the
empirical asymptotic distribution, `data1`, along with its probability
density plot:

```{r}
data <- data.frame(values=c(data0,data1), group=c(rep("empirical",B),rep("asymptotic",B)))

ggplot(data,aes(x=values,fill=group)) +
    geom_histogram(aes(y=..density..),position="identity", alpha=0.2, bins=50) +
    geom_density(alpha=.1)
```


### Invariance property of the MLE
```{r}
```

## Sum of maximum likelihood estimators

Since the variance-covariance of an MLE is inversely proportional to the
Fisher information that the MLE is defined with respect to, we can combine
multiple MLEs of $\theta$, each of which may be defined with respect to a
different kind of sample, to arrive at the MLE that incorporates the Fisher
information in all of those samples.

Consider $k$ mutually independent MLE estimators of parameter $\theta$,
$\hat\theta_1,\ldots,\hat\theta_k$, where $\hat\theta_j \sim N(\theta,I_j^{-1}(\theta))$.

Then, the maximum likelihood estimator of $\theta$ that incorporates all of
the data in $\hat\theta_1,\ldots,\hat\theta_k$ is given by the inverse-variance
weighted mean,
$$
    \hat\theta = \left(\sum I_j(\theta)\right)^{-1} \left(\sum I_j(\theta) \theta_j\right).
$$
