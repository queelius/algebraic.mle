<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="algebraic.mle">
<title>Normal distribution MLE â€¢ algebraic.mle</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Normal distribution MLE">
<meta property="og:description" content="algebraic.mle">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">algebraic.mle</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.9.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/exponential-mle.html">Exponential distribution MLE</a>
    <a class="dropdown-item" href="../articles/normal-mle.html">Normal distribution MLE</a>
    <a class="dropdown-item" href="../articles/unknow_dgp.html">Fitting models to unknown DGPs</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/queelius/algebraic.mle/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Normal distribution MLE</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/queelius/algebraic.mle/blob/HEAD/vignettes/normal-mle.Rmd" class="external-link"><code>vignettes/normal-mle.Rmd</code></a></small>
      <div class="d-none name"><code>normal-mle.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>To demonstrate the R package <code>algebraic.mle</code>, we consider
the relatively simple case of a random sample from i.i.d. normally
distributed random variables. First, we load the R package
<code>algebraic.mle</code> with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/queelius/algebraic.mle" class="external-link">algebraic.mle</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make it so that i only show 3 digits after the decimal point</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="generating-a-sample">Generating a sample<a class="anchor" aria-label="anchor" href="#generating-a-sample"></a>
</h2>
<p>We define the parameters of the i.i.d. random sample with:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">50</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p>We generate a random sample <span class="math inline">\(X_i \sim
\operatorname{N}(\mu=4,\sigma^2=2)\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> with:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>,mean<span class="op">=</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,sd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>We have observed a sample of size <span class="math inline">\(n=50\)</span>. We show some observations from this
sample (data frame) with:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">x</span>,n<span class="op">=</span><span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.98 5.67 2.45 3.99</span></span></code></pre></div>
<p>We show a histogram of the sample with:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">hist</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<p><img src="normal-mle_files/figure-html/unnamed-chunk-5-1.png" width="700"></p>
<p>We see the characteristic bell shaped curve of the normal
distribution. If we did not a prior know that the data was normally
distributed, this would be evidence that the normal distribution is a
good fit to the data.</p>
</div>
<div class="section level2">
<h2 id="maximum-likelihood-estimation">Maximum likelihood estimation<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation"></a>
</h2>
<p>If we would like to estimate <span class="math inline">\(\theta=(4,
2)'\)</span>, we can do so using maximum likelihood estimation as
implemented by the <code>algebraic.mle</code> package:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_normal_mu_var.html">mle_normal_mu_var</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt; Maximum likelihood estimator of type mle_normal_mu_var is normally distributed.</span></span>
<span><span class="co">#&gt; The estimates of the parameters are given by:</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 3.88 2.07 </span></span>
<span><span class="co">#&gt; The fisher information matrix (FIM) is given by:</span></span>
<span><span class="co">#&gt;      [,1] [,2]</span></span>
<span><span class="co">#&gt; [1,] 24.1 0.00</span></span>
<span><span class="co">#&gt; [2,]  0.0 5.81</span></span>
<span><span class="co">#&gt; The variance-covariance matrix of the estimator is given by:</span></span>
<span><span class="co">#&gt;        [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0415 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.0000 0.172</span></span>
<span><span class="co">#&gt; The asymptotic 95% confidence interval of the parameters are given by:</span></span>
<span><span class="co">#&gt;     2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  3.54  4.21</span></span>
<span><span class="co">#&gt; var 1.39  2.76</span></span>
<span><span class="co">#&gt; The bias of the estimator is given by:</span></span>
<span><span class="co">#&gt;             var </span></span>
<span><span class="co">#&gt;  0.0000 -0.0415 </span></span>
<span><span class="co">#&gt; The MSE of the estimator is  0.215 .</span></span>
<span><span class="co">#&gt; The log-likelihood is  -89.2 .</span></span>
<span><span class="co">#&gt; The AIC is  182 .</span></span>
<span><span class="co">#&gt; The standard error is  0.204 0.415 .</span></span></code></pre></div>
<p>We can show the point estimate given data <code>x</code> with:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 3.88 2.07</span></span></code></pre></div>
<p>We can show the Fisher information matrix (FIM) and
variance-covariance matrix with:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/fim.html">fim</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;      [,1] [,2]</span></span>
<span><span class="co">#&gt; [1,] 24.1 0.00</span></span>
<span><span class="co">#&gt; [2,]  0.0 5.81</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;        [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0415 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.0000 0.172</span></span></code></pre></div>
<p>We can show the confidence intervals with:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;     2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  3.54  4.21</span></span>
<span><span class="co">#&gt; var 1.39  2.76</span></span></code></pre></div>
<p>A nice property of MLEs is that, asymptotically, they converge to a
normal distribution with a mean given by the true parameter, in this
case <span class="math inline">\(\theta = (\mu,\sigma^2)'\)</span>,
and a variance-covariance given by the inverse of the Fisher information
matrix evaluated at <span class="math inline">\(\theta\)</span>. This is
how we estimated the confidence intervals and other statistics
above.</p>
<div class="section level3">
<h3 id="performance-measures-of-the-mle">Performance measures of the MLE<a class="anchor" aria-label="anchor" href="#performance-measures-of-the-mle"></a>
</h3>
<p>We do not know <span class="math inline">\(\theta\)</span>, but we
may estimate it from a sample, and thus we may approximate the sampling
distribution of <span class="math inline">\(\hat\theta\)</span> with
<span class="math inline">\(\mathcal{N}(\hat\theta,I^{-1}(\hat\theta))\)</span>.</p>
<p>Let <span class="math inline">\(F\)</span> denote the true
distribution function such that <span class="math inline">\(X_j \sim
F\)</span> for all <span class="math inline">\(j\)</span>. Suppose we
have some population parameter <span class="math inline">\(\theta =
t(F)\)</span> and an estimator of <span class="math inline">\(\theta\)</span> given by <span class="math inline">\(\hat\theta = s(\{X_1,\ldots,X_n\})\)</span>. A
reasonable requirement for an estimator <span class="math inline">\(\hat\theta\)</span> is that it converges to the
true parameter value <span class="math inline">\(\theta\)</span> as we
collect more and more data. In particular, we say that it is a
consistent estimator of <span class="math inline">\(\theta\)</span> if
<span class="math inline">\(\hat\theta\)</span> converges in probability
to <span class="math inline">\(\theta\)</span>, denoted by <span class="math inline">\(\hat\theta \overset{p}{\mapsto}
\theta\)</span>.</p>
<p>If the regularity conditions hold for the MLE, then <span class="math inline">\(\hat\theta\)</span> is a consistent estimator of
<span class="math inline">\(\theta\)</span>. However, for finite sample
sizes, the estimator may be biased. The bias of <span class="math inline">\(\hat\theta\)</span> with respect to <span class="math inline">\(\theta\)</span> is defined as <span class="math display">\[
    \operatorname{bias}(\hat\theta,\theta) = E(\hat\theta) - \theta,
\]</span> where <span class="math inline">\(\operatorname{bias}(\hat\theta,\theta) =
0\)</span> indicates that <span class="math inline">\(\hat\theta\)</span> is an <em>unbiased</em>
estimator of <span class="math inline">\(\theta\)</span>.</p>
<p>As a function of the true distribution <span class="math inline">\(F\)</span>, the bias is unknown and is not a
statistic. However, in the case of the normal, <span class="math inline">\(\hat\mu\)</span> is unbiased and, analytically,
the bias of <span class="math inline">\(\hat\sigma^2\)</span> is given
by <span class="math inline">\(-\frac{1}{n} \sigma^2\)</span>:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.hat</span>,<span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1]  0.00 -0.04</span></span></code></pre></div>
<p>If <span class="math inline">\(\sigma^2\)</span> is not known, we may
estimate it by using replacing <span class="math inline">\(\hat\sigma^2\)</span> instead:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;             var </span></span>
<span><span class="co">#&gt;  0.0000 -0.0415</span></span></code></pre></div>
<p>The mean squared error (MSE) is another performance measure of an
estimator. It is given by <span class="math display">\[
    \operatorname{mse}(\hat\theta) = E\bigl\{(\hat\theta -
\theta)^T(\hat\theta - \theta)\bigr\},
\]</span> Another way to compute the MSE is given by <span class="math display">\[
    \operatorname{mse}(\hat\theta) =
        \operatorname{trace}(\operatorname{cov}(\hat\theta) +
        \operatorname{bias}(\hat\theta)^T
        \operatorname{bias}(\hat\theta).
\]</span></p>
<p>Hereâ€™s R code to compute the MSE of <span class="math inline">\(\hat\theta\)</span>:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span>        <span class="co"># estimate of MSE</span></span>
<span><span class="co">#&gt; [1] 0.215</span></span>
<span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">theta.hat</span>,<span class="va">theta</span><span class="op">)</span>  <span class="co"># true MSE</span></span>
<span><span class="co">#&gt; [1] 0.215</span></span></code></pre></div>
<p>This looks to be a pretty good estimate of the true MSE.</p>
</div>
</div>
<div class="section level2">
<h2 id="invariance-property-of-the-mle">Invariance property of the MLE<a class="anchor" aria-label="anchor" href="#invariance-property-of-the-mle"></a>
</h2>
<p>An interesting property of an MLE <span class="math inline">\(\hat\theta\)</span> is that the MLE of <span class="math inline">\(f(\theta)\)</span> is given by <span class="math inline">\(f(\hat\theta)\)</span>. What is the distribution
of <span class="math inline">\(f(\hat\theta)\)</span>? Asymptotically,
it is normally distributed with a mean given by <span class="math inline">\(f(\theta)\)</span> and a variace-covariance given
by the covariance of the sampling distribution of <span class="math inline">\(f(\hat\theta)\)</span>. We provide two methods to
compute the variance-covariance.</p>
<div class="section level3">
<h3 id="delta-method">Delta method<a class="anchor" aria-label="anchor" href="#delta-method"></a>
</h3>
<p>If <span class="math inline">\(f\)</span> is differentiable, the
variance-covariance is given by <span class="math display">\[
\operatorname{var}(f(\hat\theta)) = \operatorname{E}\bigl\{
    \bigl(f(\hat\theta) - f(\theta)\bigr)^2\bigr\} =
    \operatorname{E}\bigl\{J_f(\hat\theta) I(\hat\theta)^{-1}
J_f(\hat\theta)^T\bigr\}.
\]</span> Here, <span class="math inline">\(J_f(\hat\theta)\)</span> is
the Jacobian of <span class="math inline">\(f\)</span> evaluated at
<span class="math inline">\(\hat\theta\)</span>.</p>
</div>
<div class="section level3">
<h3 id="monte-carlo-method">Monte-carlo method<a class="anchor" aria-label="anchor" href="#monte-carlo-method"></a>
</h3>
<p>The delta method requires that <span class="math inline">\(f\)</span>
be differentiable, but we may use the Monte-carlo method to estimate the
distribution of <span class="math inline">\(f(\hat\theta)\)</span> for
any function <span class="math inline">\(f\)</span>. We simply sample
from the MLE of <span class="math inline">\(\hat\theta\)</span> and
apply <span class="math inline">\(f\)</span> to its point estimates and
take the covariance of the sample.</p>
<p>Next, we show how to compute the sampling distribution of <span class="math inline">\(g(\hat\theta)\)</span> for some function <span class="math inline">\(g\)</span> and some MLE <span class="math inline">\(\hat\theta\)</span> using both the delta and mc
methods.</p>
</div>
<div class="section level3">
<h3 id="example-1">Example 1<a class="anchor" aria-label="anchor" href="#example-1"></a>
</h3>
<p>Let <span class="math inline">\(g(\theta) = A \theta + b\)</span> for
some matrix <span class="math inline">\(A\)</span> and vector <span class="math inline">\(b\)</span>. (This is a simple linear
transformation of <span class="math inline">\(\theta\)</span>.) We can
define <span class="math inline">\(g\)</span> in R with:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">3</span><span class="op">)</span>,nrow<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">g</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span> <span class="va">A</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">theta</span> <span class="op">+</span> <span class="va">b</span></span></code></pre></div>
<p>We compute the MLE of <span class="math inline">\(g(\theta)\)</span>
with:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">g.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/rmap.html">rmap</a></span><span class="op">(</span><span class="va">theta.hat</span>,<span class="va">g</span>,n<span class="op">=</span><span class="fl">100000L</span><span class="op">)</span></span>
<span><span class="va">g.delta</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/rmap.html">rmap</a></span><span class="op">(</span><span class="va">theta.hat</span>,<span class="va">g</span>,method<span class="op">=</span><span class="st">"delta"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">g.mc</span><span class="op">)</span>,digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]  [,3] [,4]</span></span>
<span><span class="co">#&gt; [1,] 0.166 0.249 0.000 0.00</span></span>
<span><span class="co">#&gt; [2,] 0.249 0.373 0.000 0.00</span></span>
<span><span class="co">#&gt; [3,] 0.000 0.000 0.692 1.04</span></span>
<span><span class="co">#&gt; [4,] 0.000 0.000 1.037 1.56</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">g.delta</span><span class="op">)</span>,digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]  [,3] [,4]</span></span>
<span><span class="co">#&gt; [1,] 0.166 0.249 0.000 0.00</span></span>
<span><span class="co">#&gt; [2,] 0.249 0.373 0.000 0.00</span></span>
<span><span class="co">#&gt; [3,] 0.000 0.000 0.689 1.03</span></span>
<span><span class="co">#&gt; [4,] 0.000 0.000 1.033 1.55</span></span></code></pre></div>
<p>They are pretty close.</p>
</div>
</div>
<div class="section level2">
<h2 id="weighted-mle-a-weighted-sum-of-maximum-likelihood-estimators">Weighted MLE: a weighted sum of maximum likelihood estimators<a class="anchor" aria-label="anchor" href="#weighted-mle-a-weighted-sum-of-maximum-likelihood-estimators"></a>
</h2>
<p>Since the variance-covariance of an MLE is inversely proportional to
the Fisher information that the MLE is defined with respect to, we can
combine multiple MLEs of <span class="math inline">\(\theta\)</span>,
each of which may be defined with respect to a different kind of sample,
to arrive at the MLE that incorporates the Fisher information in all of
those samples.</p>
<p>Consider <span class="math inline">\(k\)</span> mutually independent
MLEs of parameter <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat\theta_1,\ldots,\hat\theta_k\)</span>, where
<span class="math inline">\(\hat\theta_j \sim
N(\theta,I_j^{-1}(\theta))\)</span>. Then, the sampling MLE of <span class="math inline">\(\theta\)</span> that incorporates all of the data
in <span class="math inline">\(\hat\theta_1,\ldots,\hat\theta_k\)</span>
is given by the inverse-variance weighted mean, <span class="math display">\[
    \hat\theta_w = \left(\sum_{j=1}^k I_j(\theta)\right)^{-1}
\left(\sum_{j=1}^k I_j(\theta) \hat\theta_j\right),
\]</span> which, asymptotically, has an expected value of <span class="math inline">\(\theta\)</span> and a variance-covariance of <span class="math inline">\(\left(\sum_{j=1}^k
I_j(\theta)\right)^{-1}\)</span>.</p>
<div class="section level3">
<h3 id="example-2">Example 2<a class="anchor" aria-label="anchor" href="#example-2"></a>
</h3>
<p>To evaluate the performance of the weighted MLE, we generate a sample
of <span class="math inline">\(N=1000\)</span> observations from <span class="math inline">\(\mathcal{N}(\theta)\)</span> and compute the MLE
for the observed sample, denoted by <span class="math inline">\(\hat\theta\)</span>.</p>
<p>We then divide the observed sample into <span class="math inline">\(r=5\)</span> sub-samples, each of size <span class="math inline">\(N/r=100\)</span>, and compute the MLE for each
sub-sampled, denoted by <span class="math inline">\(\theta^{(1)},\ldots,\theta^{(r)}\)</span>.</p>
<p>Finally, we do a weighted combination these MLEs to form the weighted
MLE, denoted by <span class="math inline">\(\theta_w\)</span>:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span><span class="va">samp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">N</span>,mean<span class="op">=</span><span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,sd<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">samp.sub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="va">samp</span>,nrow<span class="op">=</span><span class="va">r</span><span class="op">)</span></span>
<span><span class="va">mles.sub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>length<span class="op">=</span><span class="va">r</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">r</span><span class="op">)</span></span>
<span>    <span class="va">mles.sub</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_normal_mu_var.html">mle_normal_mu_var</a></span><span class="op">(</span><span class="va">samp.sub</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mle.wt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_weighted.html">mle_weighted</a></span><span class="op">(</span><span class="va">mles.sub</span><span class="op">)</span></span>
<span><span class="va">mle</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_normal_mu_var.html">mle_normal_mu_var</a></span><span class="op">(</span><span class="va">samp</span><span class="op">)</span></span></code></pre></div>
<p>We show the results in the following R code. First, we show the
weighted MLE and its MSE:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="va">mle.wt</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 4.07 1.95</span></span>
<span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">mle.wt</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.00956</span></span></code></pre></div>
<p>The MLE for the total sample and its MSE is:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="va">mle</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 4.07 1.96</span></span>
<span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">mle</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.00968</span></span></code></pre></div>
<p>We see that <span class="math inline">\(\hat\theta\)</span> and <span class="math inline">\(\hat\theta_w\)</span> model approximately the same
sampling distribution when estimating <span class="math inline">\(\theta\)</span> given i.i.d. samples.</p>
</div>
</div>
<div class="section level2">
<h2 id="bootstrapping-the-mles">Bootstrapping the MLEs<a class="anchor" aria-label="anchor" href="#bootstrapping-the-mles"></a>
</h2>
<p>Letâ€™s compare the earlier results that relied on the large sampling
assumption with the bootstrapped MLE using <code>mle_boot</code>. First,
<code>mle_boot</code> needs an MLE solver for the normal distribution.
We have one with <code>mle_normal_mu_var</code>: we just need to wrap it
in a function that takes the data as input and returns the MLE of the
parameters and then pass it to <code>mle_boot</code> constructor:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mle_solver</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">ind</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="fu"><a href="../reference/mle_normal_mu_var.html">mle_normal_mu_var</a></span><span class="op">(</span><span class="va">data</span><span class="op">[</span><span class="va">ind</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">R</span> <span class="op">&lt;-</span> <span class="fl">1000</span> <span class="co"># number of bootstrap replicates</span></span>
<span><span class="co"># recall the data is `x`</span></span>
<span><span class="op">(</span><span class="va">theta.boot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_boot.html">mle_boot</a></span><span class="op">(</span><span class="va">mle_solver</span>, <span class="va">x</span>, <span class="va">R</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Maximum likelihood estimator of type mle_boot is normally distributed.</span></span>
<span><span class="co">#&gt; The estimates of the parameters are given by:</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 3.88 2.07 </span></span>
<span><span class="co">#&gt; The fisher information matrix (FIM) is given by:</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 29.56 -6.45</span></span>
<span><span class="co">#&gt; [2,] -6.45  9.56</span></span>
<span><span class="co">#&gt; The variance-covariance matrix of the estimator is given by:</span></span>
<span><span class="co">#&gt;        [,1]   [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0397 0.0268</span></span>
<span><span class="co">#&gt; [2,] 0.0268 0.1227</span></span>
<span><span class="co">#&gt; The asymptotic 95% confidence interval of the parameters are given by:</span></span>
<span><span class="co">#&gt;     2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  3.55  4.20</span></span>
<span><span class="co">#&gt; var 1.50  2.65</span></span>
<span><span class="co">#&gt; The bias of the estimator is given by:</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00961 -0.03538 </span></span>
<span><span class="co">#&gt; The MSE of the estimator is  0.164 .</span></span>
<span><span class="co">#&gt; The log-likelihood is  .</span></span>
<span><span class="co">#&gt; The AIC is   .</span></span>
<span><span class="co">#&gt; The standard error is  0.199 0.35 .</span></span></code></pre></div>
<p>We already printed out the <code>theta.boot</code> object, which
provided a lot of information about it, but we can obtain specified
statistics from the Bootstrap MLE using the standard interface in
<code>algorithmic.mle</code>, e.g.:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span><span class="op">)</span> <span class="co"># point estimate</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 3.88 2.07</span></span>
<span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span><span class="op">)</span> <span class="co"># variance-covariance matrix</span></span>
<span><span class="co">#&gt;        [,1]   [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0397 0.0268</span></span>
<span><span class="co">#&gt; [2,] 0.0268 0.1227</span></span>
<span><span class="op">(</span><span class="fu"><a href="../reference/se.html">se</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span><span class="op">)</span> <span class="co"># standard error</span></span>
<span><span class="co">#&gt; [1] 0.199 0.350</span></span>
<span><span class="op">(</span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span><span class="op">)</span> <span class="co"># bias</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00961 -0.03538</span></span>
<span><span class="op">(</span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span><span class="op">)</span> <span class="co"># mean squared error</span></span>
<span><span class="co">#&gt; [1] 0.164</span></span></code></pre></div>
<p>We see that, for the most part, the results are similar to those
obtained using the large sampling assumption.</p>
<p>There are many additional arguments you can pass to the
<code>mle_boot</code> function. For instance, parallel computing may be
used to speed up the bootstrap process. For this, you need to load the
<code>parallel</code> package and set the parallel arguments for the
<code>mle_boot</code> function. For example, to use 4 cores, you would
do the following (note that we do not evaluate this code in the vignette
to reduce the number of dependencies):</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">parallel</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/mle_boot.html">mle_boot</a></span><span class="op">(</span><span class="va">mle_solver</span>, <span class="va">x</span>, <span class="va">R</span>, parallel <span class="op">=</span> <span class="st">"multicore"</span>, ncpus <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prediction-intervals">Prediction intervals<a class="anchor" aria-label="anchor" href="#prediction-intervals"></a>
</h2>
<p>Frequently, we are actually interested in predicting the outcome of
the random variable (or vector) that we are estimating the parameters
of.</p>
<p>We observed a sample <span class="math inline">\(\mathcal{D} =
\{T_i\}_{i=1}^n\)</span> where <span class="math inline">\(T_i \sim
N(\mu,\sigma^2)\)</span>, <span class="math inline">\(\theta =
(\mu,\sigma^2)^T\)</span> is not known. We compute the MLE of <span class="math inline">\(\theta\)</span>, which, asymptotically, is
normally distributed with a mean <span class="math inline">\(\theta\)</span> and a variance-covariance <span class="math inline">\(I^{-1}(\theta)/n\)</span>.</p>
<p>We wish to model the uncertainty of a new observation, <span class="math inline">\(\hat{T}_{n+1}|\mathcal{D}\)</span>. We do so by
considering both the uncertainty inherent to the Normal distribution and
the uncertainty of our estimate <span class="math inline">\(\hat\theta\)</span> of <span class="math inline">\(\theta\)</span>. In particular, we let <span class="math inline">\(\hat{T}_{n+1}|\hat\theta \sim
N(\hat\mu,\hat\sigma^2)\)</span> and <span class="math inline">\(\hat\theta \sim
N(\theta,I^{-1}(\theta)/n)\)</span> (the sampling distribution of the
MLE). Then, the joint distribution of <span class="math inline">\(\hat{T}_{n+1}\)</span> and <span class="math inline">\(\hat\theta\)</span> has the pdf given by <span class="math display">\[
    f(t,\theta) = f_{\hat{T}|\hat\theta}(t|\theta=(\mu,\sigma^2))
f_{\hat\theta}(\theta),
\]</span> and thus to find <span class="math inline">\(f(t)\)</span>, we
marginalize over <span class="math inline">\(\theta\)</span>, obtaining
<span class="math display">\[
    f(t) = \int_{-\infty}^\infty \int_{-\infty}^{\infty}
f_{\hat{T}_{n+1},\hat\mu,\hat\sigma^2}(t,\mu,\sigma^2) d\mu d\sigma^2.
\]</span></p>
<p>Given the information in the sample, the uncertainty in the new
observation is characterized by the distribution <span class="math display">\[
    \hat{T}_{n+1} \sim f(t).
\]</span></p>
<p>It has greater variance than <span class="math inline">\(T_{n+1}|\hat\theta\)</span> because, as stated
earlier, we do not know <span class="math inline">\(\theta\)</span>, we
only have an uncertain estimate <span class="math inline">\(\hat\theta\)</span>.</p>
<p>In <code>pred</code>, we compute the predictive interval (PI) of the
distribution of <span class="math inline">\(\hat{T}_{n+1}\)</span> using
Monte Carlo simulation, where we replace the integral with a sum over a
large number of draws from the joint distribution of <span class="math inline">\(\hat{T}_{n+1}\)</span> and <span class="math inline">\(\hat\theta\)</span> and then compute the empirical
quantiles.</p>
<p>The function <code>pred</code> takes as arguments <code>x</code>, in
this case an <code>mle</code> object, and a sampler for the distribution
of the random variable of interest, in this case <code>rnorm</code> (the
sampler for the normal distribution). The sampler must be compatible
with the output of <code>point(x)</code>, whether that output be a
scalar or a vector. Here is how we compute the PI for <span class="math inline">\(\hat{T}_{n+1}\)</span>:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/pred.html">pred</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">theta.hat</span>, samp<span class="op">=</span><span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">=</span><span class="fl">1</span>,<span class="va">theta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,<span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;      mean  lower upper</span></span>
<span><span class="co">#&gt; [1,] 3.89 -0.342  8.15</span></span></code></pre></div>
<p>In general, it will return a <span class="math inline">\(p\)</span>-by-<span class="math inline">\(3\)</span> matrix, where <span class="math inline">\(p\)</span> is the dimension of <span class="math inline">\(T\)</span> and the columns are the mean, lower
quantile, and upper quantile of the predictive distribution.</p>
<p>How does this compare to <span class="math inline">\(T_{n+1}|\hat\theta\)</span>? We can compute the
95% quantile interval for <span class="math inline">\(T_{n+1}|\hat\theta\)</span> using the
<code>qnorm</code> function:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">sd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="../reference/point.html">point</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span>mean<span class="op">=</span><span class="va">mu</span>,lower<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">qnorm</a></span><span class="op">(</span><span class="fl">.025</span>,mean<span class="op">=</span><span class="va">mu</span>, sd<span class="op">=</span><span class="va">sd</span><span class="op">)</span>,upper<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">qnorm</a></span><span class="op">(</span><span class="fl">.975</span>,mean<span class="op">=</span><span class="va">mu</span>, sd<span class="op">=</span><span class="va">sd</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; mean.mu   lower   upper </span></span>
<span><span class="co">#&gt;    3.88    1.05    6.70</span></span></code></pre></div>
<p>We see that the 95% quantile interval for <span class="math inline">\(T_{n+1}|\hat\theta\)</span> is smaller than <span class="math inline">\(\hat{T}_{n+1}\)</span>, which is what we expected.
After all, there is uncertainty about the parameter value <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>In this vignette, we demonstrated how to use the
<code>algebraic.mle</code> package to estimate the sampling distribution
of the MLE using the large sampling assumption and the Bootstrap method.
The package provides various functions for obtaining statistics of the
MLE, allowing for a deeper understanding of the properties of your
estimator.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Alexander Towell.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
