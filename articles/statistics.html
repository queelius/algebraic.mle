<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="algebraic.mle">
<title>Statistics and characteristics of the MLE • algebraic.mle</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Statistics and characteristics of the MLE">
<meta property="og:description" content="algebraic.mle">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">algebraic.mle</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.9.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/dgp.html">Dynamic failure rate model</a>
    <a class="dropdown-item" href="../articles/statistics.html">Statistics and characteristics of the MLE</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/queelius/algebraic.mle/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Statistics and characteristics of the MLE</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/queelius/algebraic.mle/blob/HEAD/vignettes/statistics.Rmd" class="external-link"><code>vignettes/statistics.Rmd</code></a></small>
      <div class="d-none name"><code>statistics.Rmd</code></div>
    </div>

    
    
<!-- 
> If MLEs had different sampling distributions (e.g., depend on different kinds
> of samples, like right-censored samples or different sized samples), we would
> more optimally use a weighted average of the MLEs (see `mle_weighted`), but in this
> case, the variance-covariance matrices are identical, so we can use the sample
> of MLEs as an approximation of the sampling distribution of the MLEs and take
> simple statistics of the sample of MLEs to get any desired estimate of the
> population parameter vector $(\mu, \sigma^2)'$. -->
<!-- badges: start -->
<p><a href="https://github.com/queelius/algebraic.mle/blob/master/LICENSE" class="external-link"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a> <!-- badges: end --></p>
<p><code>algebraic.mle</code> is an R package that provides an algebra
over Maximum Likelihood Estimators (MLEs). These estimators possess many
desirable, well-defined statistical properties which the package helps
you manipulate and utilize.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>The R package <code>algebraic.mle</code> can be installed from GitHub
by using the devtools package in R:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"queelius/algebraic.mle"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="normal-distribution">Normal distribution<a class="anchor" aria-label="anchor" href="#normal-distribution"></a>
</h2>
<p>We are going to the classic Normal distribution to demonstrate how to
use <code>algebraic.mle</code>. We are using it for a few reasons:</p>
<ol style="list-style-type: decimal">
<li>It’s well-understood, so we can compare our results to the known
results.</li>
<li>It’s a very common distribution, so it’s useful to have a good
understanding of its properties.</li>
<li>The MLE is multivariate, so we can demonstrate how to use
<code>algebraic.mle</code> for multivariate distributions.</li>
</ol>
<p>So, first, we define a simple MLE solver for the normal
distribution.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_normal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">data</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">loglik</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>        <span class="op">-</span><span class="va">n</span> <span class="op">/</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span></span>
<span>            <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">data</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">+</span> <span class="va">n</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">par.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span>mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, var <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html" class="external-link">sigma</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">H</span> <span class="op">&lt;-</span> <span class="fu">numDeriv</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/hessian.html" class="external-link">hessian</a></span><span class="op">(</span>func <span class="op">=</span> <span class="va">loglik</span>, x <span class="op">=</span> <span class="va">par.hat</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span>    <span class="fu">algebraic.mle</span><span class="fu">::</span><span class="fu"><a href="../reference/mle.html">mle</a></span><span class="op">(</span></span>
<span>        theta.hat <span class="op">=</span> <span class="va">par.hat</span>,</span>
<span>        loglike <span class="op">=</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">par.hat</span>, <span class="va">data</span><span class="op">)</span>,</span>
<span>        score <span class="op">=</span> <span class="fu">numDeriv</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/grad.html" class="external-link">grad</a></span><span class="op">(</span>func <span class="op">=</span> <span class="va">loglik</span>, x <span class="op">=</span> <span class="va">par.hat</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span>,</span>
<span>        sigma <span class="op">=</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/ginv.html" class="external-link">ginv</a></span><span class="op">(</span><span class="op">-</span><span class="va">H</span><span class="op">)</span>,</span>
<span>        info <span class="op">=</span> <span class="op">-</span><span class="va">H</span>,</span>
<span>        obs <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>        nobs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>,</span>
<span>        superclasses <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mle_normal"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>As you can see, we return an <code>mle</code> object, and then we
give it a sub-class <code>mle_normal</code> (it is also a subclass of
<code>mle</code> and <code>algebraic.dist</code>’s <code>dist</code>) so
we can specialize some of the methods for MLE of the normal
distribution, e.g., <code>bias.mle_normal</code> which we show
later.</p>
</div>
<div class="section level2">
<h2 id="monte-carlo-mc-simulation-of-the-sampling-distribution-of-the-mle">Monte-carlo (MC) simulation of the sampling distribution of the
MLE<a class="anchor" aria-label="anchor" href="#monte-carlo-mc-simulation-of-the-sampling-distribution-of-the-mle"></a>
</h2>
<p>Let’s define <code>theta_samp_mc</code>, which stands for the Monte
Carlo simulation of the sampling distribution of the MLE. It takes a
sample size <code>n</code>, a true parameter value <code>theta</code>,
and a number of simulations <code>B</code> to run. It returns a matrix
with <code>B</code> rows and two columns, the first column is the MLE of
the mean and the second column is the MLE of the variance.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta_samp_mc</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">theta</span>, <span class="va">B</span> <span class="op">=</span> <span class="fl">10000</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">mu</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>    <span class="va">var</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>    <span class="va">mles</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="va">B</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">B</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">mles</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="fu">fit_normal</span><span class="op">(</span><span class="va">d</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">mles</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"var"</span><span class="op">)</span></span>
<span>    <span class="va">mles</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set up the parameters of a simulation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">913254</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">70</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">var</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mu</span>, <span class="va">var</span><span class="op">)</span></span>
<span><span class="va">mles</span> <span class="op">&lt;-</span> <span class="fu">theta_samp_mc</span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, theta <span class="op">=</span> <span class="va">theta</span>, B <span class="op">=</span> <span class="va">B</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">mles</span><span class="op">)</span></span>
<span><span class="co">#&gt;         mu   var</span></span>
<span><span class="co">#&gt; [1,] 0.891 0.728</span></span>
<span><span class="co">#&gt; [2,] 1.033 1.102</span></span>
<span><span class="co">#&gt; [3,] 1.105 1.094</span></span>
<span><span class="co">#&gt; [4,] 1.146 0.832</span></span>
<span><span class="co">#&gt; [5,] 0.916 0.895</span></span>
<span><span class="co">#&gt; [6,] 0.986 0.904</span></span></code></pre></div>
<p>The matrix <code>mles</code> is a sample of MLEs from the sampling
distribution of the MLE. It is an <em>empirical distribution</em> of the
MLE <span class="math inline">\((\mu, \sigma^2)'\)</span> from
samples of size <span class="math inline">\(n\)</span> <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>.</p>
<p>This particular example is Monte Carlo simulation of the sampling
distribution, since we are simulating the sampling distribution by
repeatedly sampling from the population distribution and computing the
MLE for each sample.</p>
<blockquote>
<p>In bootstrap, we would <em>resample</em> from the sample, not the
population, but with a large enough sample, the two will produce nearly
identical results. See the bootstrap section for more details, where
we’ll compare the two.</p>
</blockquote>
<p>For a sufficiently large number of simulations <span class="math inline">\(B\)</span>, the empirical sampling distribution
should be very close to the true sampling distribution. We can plot the
empirical sampling distribution of the MLEs using the <code>plot</code>
function on the <code>mles</code> matrix.</p>
<div class="figure" style="text-align: center">
<img src="reference/figures/README-unnamed-chunk-3-1.png" alt="Sampling distribution of the MLEs." width="100%"><p class="caption">
Sampling distribution of the MLEs.
</p>
</div>
<p>In <code>algebraic.dist</code>, we can use
<code>empirical_dist</code> to represent an empirical sampling
distribution by giving it the sample of MLEs previously generated:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta.mc</span> <span class="op">&lt;-</span> <span class="fu">algebraic.dist</span><span class="fu">::</span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/empirical_dist.html" class="external-link">empirical_dist</a></span><span class="op">(</span><span class="va">mles</span><span class="op">)</span></span></code></pre></div>
<p>In general, for any MLE and assuming the the regularity conditions
hold, the asymptotic sampling distribution of the MLE is normal with
mean <span class="math inline">\(\theta\)</span> and variance-covariance
matrix <span class="math inline">\(\Sigma = I^{-1}(\theta)_n\)</span>,
where <span class="math inline">\(I\)</span> is the Fisher information
matrix and <span class="math inline">\(n\)</span> is the sample size.
However, in general:</p>
<ol style="list-style-type: decimal">
<li>We don’t know when the asymptotic sampling distribution is a good
approximation to the true sampling distribution. In these cases, the
empirical sampling distribution may be used instead.</li>
<li>We may not be confident our implementation of the MLE is correct, in
which case the empirical sampling distribution can be used to check our
implementation.</li>
<li>The regularity conditions may not hold, in which case the asymptotic
sampling distribution may not be known. In these cases, the empirical
sampling distribution may be used instead.</li>
</ol>
<p>With these caveats in mind, we compare some of the statistics of the
empirical sampling distribution of the MLE for the normal distribution
and the asymptotic sampling distribution.</p>
<p>Let’s look at some basic parameters of the sampling distribution of
the MLE for the normal distribution. First, let’s look at the mean:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">mu.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 0.999 0.988</span></span></code></pre></div>
<p>The mean looks pretty close to the true parameter vector <span class="math display">\[
    \theta = (\mu = 1, \sigma^2 = 1)'.
\]</span></p>
<p>We can actually compute any parameter, since <code>theta.mc</code>,
models the concept of a distribution. In particular, it models a
distribution in <code>algebraic.dist</code>, and thus the API exposed by
<code>algebraic.dist</code> is available to us. For instance, we can
compute various parameters of the sampling distribution of the MLE using
the <code>expectation</code> function:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># should sum to 1</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co"># mean</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 0.999 0.988</span></span>
<span><span class="co"># variance of (mu, var)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt;     mu    var </span></span>
<span><span class="co">#&gt; 0.0142 0.0289</span></span>
<span><span class="co"># kurtosis of (mu, var) </span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span><span class="op">^</span><span class="fl">4</span><span class="op">)</span> <span class="op">/</span></span>
<span>    <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 3.01 3.11</span></span>
<span><span class="co"># skewness of mu and var -- should be (0, 0)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span> <span class="op">/</span> <span class="va">theta</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;         mu        var </span></span>
<span><span class="co">#&gt; -0.0000342  0.0017230</span></span>
<span><span class="co"># covariance of (mu, var) -- should be around 0</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] -0.0000648</span></span></code></pre></div>
<p>We could use the mean and variance-covariance matrix to parameterize
a multivariate normal distribution (MVN), for instance, but we don’t do
that here.</p>
</div>
<div class="section level2">
<h2 id="bias">Bias<a class="anchor" aria-label="anchor" href="#bias"></a>
</h2>
<p>Bias is a measure of the systematic error of an estimator; it
measures how far its average value is from the true value being
estimated. Formally, it is defined as the difference between the
expected value of the estimator and the true value of the parameter,
i.e., <span class="math display">\[
\operatorname{Bias}(\hat\theta) = E_{\hat\theta}(\hat\theta) - \theta,
\]</span> where <span class="math inline">\(E_{\hat\theta}\)</span>
denotes the expectation operator with respect to the sampling
distribution of <span class="math inline">\(\hat\theta\)</span>.
(Normally, we drop the subscript in the expectation operator and write
<span class="math inline">\(E\)</span> instead of <span class="math inline">\(E_{\hat\theta}\)</span> unless it’s not clear from
context which expectation operator we are using.)</p>
<p>When the bias is zero, the estimator is <em>unbiased</em>, otherwise
it is <em>biased</em>. Analytically, the asymptotic bias of the MLE for
the parameters of the normal distribution is <span class="math display">\[
    \operatorname{Bias}(\hat\theta) = \left(\begin{array}{c}
        0 \\
        -\frac{\sigma^2}{n}
    \end{array}\right).
\]</span> Plugging in the true value of <span class="math inline">\(\sigma^2 = 1\)</span> and the sample size <span class="math inline">\(n = 70\)</span>, we get <span class="math inline">\((0, 0.02857143)'\)</span>. We may also provide
an appropriate implementation of the <code>bias</code> method in
<code>algebraic.mle</code> for <code>mle_normal</code> (which is what we
called our the object that we returned from
<code>fit_normal</code>):</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bias.mle_normal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">par</span> <span class="op">=</span> <span class="cn">NULL</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">par</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">par</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span>mu <span class="op">=</span> <span class="fl">0</span>, var <span class="op">=</span> <span class="op">-</span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html" class="external-link">nobs</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Now, let’s compute the bias using this function, and the estimate of
the bias provided by the <code>bias.mle_emp</code>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># first, we sample some data from the true distribution</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># now we fit it to the normal distribution</span></span>
<span><span class="va">theta.hat</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># now we compute the bias, first using the asymptotic theory</span></span>
<span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.hat</span>, <span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt;      mu     var </span></span>
<span><span class="co">#&gt;  0.0000 -0.0143</span></span>
<span><span class="co"># now using the empirical sampling distribution</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span> <span class="co"># mean(theta.mc) - theta</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00109 -0.01245</span></span></code></pre></div>
<p>The asymptotic bias and the empirical bias are pretty close. Let’s
see how the bias of the variance changes as the sample size
increases.</p>
<details><summary>
Click to show/hide R code
</summary><div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">200</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">bias_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">vars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span>
<span>        <span class="va">vars</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">bias_var</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">vars</span><span class="op">)</span> <span class="op">-</span> <span class="va">var</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
</details><p><img src="reference/figures/README-bias-plot-1.png" width="100%"><img src="reference/figures/README-bias-plot-2.png" width="100%"></p>
<div class="section level3">
<h3 id="variance-covariance-matrix">Variance-covariance matrix<a class="anchor" aria-label="anchor" href="#variance-covariance-matrix"></a>
</h3>
<p>The variance-covariance matrix is one of the more important
statistical measures of an estimator of a parameter vector. It
quantities both the variability of the individual parameter estimates
and how they co-vary with each other.</p>
<p>The variance-covariance matrix of a parameter vector <span class="math inline">\(\theta = (\theta_1, \ldots,
\theta_p)'\)</span> is an <span class="math inline">\(n \times
n\)</span> matrix defined as <span class="math display">\[
\operatorname{Var}(\hat\theta) = E_{\hat\theta}\!\bigl[(\hat\theta -
E_{\hat\theta}(\hat\theta))
    (\hat\theta - E_{\hat\theta}(\hat\theta))'\bigr].
\]</span></p>
<p>The <span class="math inline">\((i, j)\)</span>th element of the
variance-covariance matrix is the covariance between the <span class="math inline">\(i\)</span>th and <span class="math inline">\(j\)</span>th elements of the parameter vector,
respectively <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\theta_j\)</span>. Thus, the diagonal elements of
the variance-covariance matrix are the variances of the individual
parameter estimates, and the off-diagonal elements are the covariances
between the parameter estimates.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span>, digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.019 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.052</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span>, digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;        mu   var</span></span>
<span><span class="co">#&gt; mu  0.014 0.000</span></span>
<span><span class="co">#&gt; var 0.000 0.029</span></span></code></pre></div>
<p>They look reasonably close, suggesting at <span class="math inline">\(n = 200\)</span>, the asymptotic sampling
distribution is a good approximation to the “true” sampling distribution
of the MLE.</p>
</div>
<div class="section level3">
<h3 id="confidence-intervals">Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"></a>
</h3>
<p>We can compute the CI of a parameter using the <code>confint</code>
function:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;      2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  0.533  1.09</span></span>
<span><span class="co">#&gt; var 0.892  1.80</span></span></code></pre></div>
<p>A very important measure of the accuracy of an estimator is its
coverage probability, which is the probability that the confidence
interval for the parameter estimate contains the true value of the
parameter. If the coverage probability for an <span class="math inline">\((1-\alpha) \%\)</span>-confidence interval is
<span class="math inline">\(1-\alpha\)</span>, then the confidence
interval is said to be <em>well-calibrated</em>. If the coverage
probability is less than <span class="math inline">\(1-\alpha\)</span>,
then the confidence interval is said to be <em>conservative</em>; if the
coverage probability is greater than <span class="math inline">\(1-\alpha\)</span>, then the confidence interval is
said to be <em>anti-conservative</em>.</p>
<p>We can estimate it by simulating a large number of samples from the
population distribution and computing the proportion of times the
confidence interval contains the true value of the parameter. We can do
this for both the mean and variance of the normal distribution.</p>
<details><summary>
Click to show/hide R code
</summary><div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">1000</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">coverage_prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">count1</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="va">count2</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span>
<span>        <span class="va">ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">mu</span> <span class="op">&amp;&amp;</span> <span class="va">mu</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count1</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">var</span> <span class="op">&amp;&amp;</span> <span class="va">var</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count2</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
</details><p><img src="reference/figures/README-cov-plot-1.png" width="100%"></p>
<p>We see that the coverage probability is close to the nominal coverage
probability, and converges to it as the sample size increases. This
suggests that the confidence intervals are well-calibrated.</p>
</div>
</div>
<div class="section level2">
<h2 id="mean-squared-error-matrix">Mean squared error matrix<a class="anchor" aria-label="anchor" href="#mean-squared-error-matrix"></a>
</h2>
<p>The mean squared error (MSE) of an estimator of a parameter vector
<span class="math inline">\(\theta\)</span> is defined as <span class="math display">\[
\operatorname{MSE}(\hat\theta) = E\bigl[(\hat\theta - \theta)(\hat\theta
- \theta)'\bigr],
\]</span> where <span class="math inline">\(\hat\theta - \theta\)</span>
is a column vector of differences between the estimator and the true
parameter and <span class="math inline">\((\hat\theta -
\theta)'\)</span> is a row vector of the same differences, and we
are performing a standard matrix multiplication between the two vectors.
The MSE is a measure of the average squared error of the estimator. It
is a function of the true parameter value <span class="math inline">\(\theta\)</span>.</p>
<p>This MSE is a <em>matrix</em>. It is very similar to the
variance-covariance matrix, which is defined as <span class="math display">\[
\operatorname{Var}(\hat\theta) = E\bigl[(\hat\theta - E(\hat\theta))
    (\hat\theta - E(\hat\theta))'\bigr],
\]</span> where we replace the true paramater <span class="math inline">\(\theta\)</span> with the expected value of the
estimator <span class="math inline">\(\hat\theta\)</span>. If the
estimator is unbiased, then <span class="math inline">\(E(\hat\theta) =
\theta\)</span> and <span class="math inline">\(\operatorname{Var}(\hat\theta) =
\operatorname{MSE}(\hat\theta)\)</span>.</p>
<p>We not only need to consider the estimation error for each parameter
individually, but also how these errors might relate to each other. For
instance, it could be the case that when we overestimate one parameter,
we tend to underestimate another. This kind of relationship between
errors in estimating different parameters can be captured by the
off-diagonal elements of the MSE matrix, which represent the covariances
between errors.</p>
<p>The diagonal elements of the MSE represent the MSE of the individual
parameter estimators, e.g., the <span class="math inline">\(i\)</span>th
diagonal element represents <span class="math inline">\(\operatorname{MSE}(\hat\theta_j)\)</span>.</p>
<p>The <em>trace</em> of the MSE, the sum of the diagonal elements,
represents the total MSE across all parameters. As a single summary
statistic, it may be useful for comparing different estimators.</p>
<p>The MSE can be decomposed into two parts:</p>
<ol style="list-style-type: decimal">
<li>The <em>bias</em>, which is the difference between the expected
value of the estimator and the true parameter value, and</li>
<li>The <em>variance</em>, which is the variance of the estimator.</li>
</ol>
<p>The MSE is then computed as the sum of the bias outer product and the
variance-covariance matrix:</p>
<p><span class="math display">\[
\operatorname{MSE}(\hat\theta) =
\operatorname{Bias}(\hat\theta)\operatorname{Bias}(\hat\theta)'
    + \operatorname{Var}(\hat\theta).
\]</span></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mse.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">theta.hat</span>, <span class="va">theta</span><span class="op">)</span></span>
<span><span class="va">mse.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>,</span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">mse.hat</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;         mu   var</span></span>
<span><span class="co">#&gt; [1,] 0.019 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.052</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">mse.mc</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.014 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.029</span></span></code></pre></div>
<p>It’s hard to distinguish the MSE matrices from the
variance-covariance matrices reported previously, which is not
surprising, since the bias is relatively small and so the MSE is
dominated by the variance.</p>
<p>Let’s take a closer look at the variance and MSE of the mean <span class="math inline">\(\hat\mu\)</span>:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># temporarily show more digits in the numbers/outputs for this code block</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">12</span><span class="op">)</span></span>
<span><span class="co"># mse(mu)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.014176072516</span></span>
<span><span class="co"># variance(mu)</span></span>
<span><span class="op">(</span><span class="va">mu.var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0141748829208</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span></span>
<span><span class="co"># mse = bias^2 + variance</span></span>
<span><span class="va">b</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">mu.var</span></span>
<span><span class="co">#&gt; [1] 0.014176072516</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<p>They are very close, since the bias is so small.</p>
<p>We should take the MSE from the Monte Carlo simulation as a sort of
“true” MSE, since it is computed from the empirical sampling
distribution of the MLE. We expect that as the sample size increases,
the asymptotic MSE (<code>mse.hat</code>) and the MC MSE
(<code>mse.mc</code>) will converge to the same value. In fact, let’s
run a little experiment to show this:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">200</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">mses.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mses.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mses.hat.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">theta.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/empirical_dist.html" class="external-link">empirical_dist</a></span><span class="op">(</span><span class="fu">theta_samp_mc</span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, theta <span class="op">=</span> <span class="va">theta</span>, B <span class="op">=</span> <span class="va">B</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">mse.mu.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.n</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="va">mse.var.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.n</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">var</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>    <span class="va">mses.mc</span><span class="op">[</span><span class="va">j</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mse.mu.n</span>, <span class="va">mse.var.n</span><span class="op">)</span></span>
<span>    <span class="va">mses.hat</span><span class="op">[</span><span class="va">j</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">fit</span>, <span class="va">theta</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">mses.hat.hat</span><span class="op">[</span><span class="va">j</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p><img src="reference/figures/README-mse-plots-1.png" width="100%"><img src="reference/figures/README-mse-plots-2.png" width="100%"></p>
<p>These plots demonstrate that the asymptotic MSE is a good
approximation to the “true” MSE, which is the MSE computed from the
empirical sampling distribution of the MLE.</p>
<p>It’s difficult to distinguish the estimated asymptotic MSE, where the
true parameter <span class="math inline">\(\theta\)</span> is not known,
from the asymptotic MSE, where the true parameter <span class="math inline">\(\theta\)</span> is known. This is because the bias
is so small, and so the MSE is dominated by the variance.</p>
</div>
<div class="section level2">
<h2 id="bootstrap-of-the-sampling-distribution-of-the-mle">Bootstrap of the sampling distribution of the MLE<a class="anchor" aria-label="anchor" href="#bootstrap-of-the-sampling-distribution-of-the-mle"></a>
</h2>
<p>Normally, we don’t know the true data generating process (DGP) of the
data we observe. We only have a sample of data, and we want to use that
sample to estimate the parameters of some model that hopefully provides
a good fit to the DGP using maximum likelihood estimation.</p>
<p>Earlier, we simulated a sample of data from a normal distribution
with mean 1 and variance 1 and then used MLE on each sample to generate
an empirical sampling distribution of the MLE. This is called <em>Monte
Carlo simulation</em>.</p>
<p>However, we can also use the sample of data we have to generate an
empirical sampling distribution of the MLE. This is called
<em>bootstrap</em>. The idea is that the sample we have is a sample from
the true DGP, and we can use that sample to generate new samples
(resample) and fit an MLE to each of these to generate an Bootstrapped
empirical sampling distribution of the MLE.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate a sample of n observations from a normal with mean 1 and variance 2.</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">boot</span><span class="op">)</span></span>
<span><span class="va">theta.boot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_boot.html">mle_boot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/boot.html" class="external-link">boot</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">data</span>,</span>
<span>    statistic <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">ind</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="fu">fit_normal</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">ind</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span>,</span>
<span>    R <span class="op">=</span> <span class="va">B</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Let’s compute some statistics:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 0.936 0.984</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span></span>
<span><span class="co">#&gt;      2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  0.798  1.07</span></span>
<span><span class="co">#&gt; var 0.796  1.18</span></span></code></pre></div>
<p>Let’s use these Bootstrapped MLEs to generate an aproximation of the
empirical sampling distribution:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/empirical_dist.html" class="external-link">empirical_dist</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">$</span><span class="va">t</span><span class="op">)</span></span></code></pre></div>
<p>As before, let’s do some basic expectations of the Bootstrapped
sampling distribution of the MLE and compare to the previous
results:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># should sum to 1</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co"># mean</span></span>
<span><span class="op">(</span><span class="va">mu.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">theta.b</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.937 0.978</span></span>
<span><span class="co"># variance of (mu, var)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.00488 0.00973</span></span>
<span><span class="co"># kurtosis of (mu, var) </span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span><span class="op">^</span><span class="fl">4</span><span class="op">)</span> <span class="op">/</span></span>
<span>    <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="co">#&gt; [1] 2.98 3.10</span></span>
<span><span class="co"># skewness of mu and var -- should be (0, 0)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span> <span class="op">/</span> <span class="va">theta</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] -0.00000665  0.00020805</span></span>
<span><span class="co"># covariance of (mu, var) -- should be around 0</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.000151</span></span></code></pre></div>
<p>These are not too bad.</p>
<p>Let’s compute the bias and compare it to the previous results:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt;  0.00104 -0.00556</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00109 -0.01245</span></span>
<span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.hat</span>, <span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt;      mu     var </span></span>
<span><span class="co">#&gt;  0.0000 -0.0143</span></span></code></pre></div>
<p>We see that the <code>bias</code> function for <code>mle_boot</code>
is not too bad. Note that the <code>bias</code> is an expectation w.r.t.
the sampling distribution of the MLE. In general, we can have a better
estimator if we use <span class="math display">\[
    \hat\theta^* = \hat\theta - \operatorname{Bias}(\hat\theta),
\]</span> assuming the bias estimate is accurate. In this particular
example that transformation makes it worse, which is fine, the bias of
the transformed estimator would be less in theory. Howevever, in
practice, we don’t trust the bias reported by the Bootstrap, except as
evidence that our estimator is biased or not. The analytic bias,
<code>bias.mle_normal</code>, is more accurate, and will generally
produce estimators with less bias (although by the bias-variance
trade-off, it may have more variance).</p>
<p>Let’s compare the variance-covariance matrix of the Bootstrapped
sampling distribution of the MLE to the “true” sampling distribution and
the asymptotic sampling distribution:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.b</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1] [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.005 0.00</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.01</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;        mu   var</span></span>
<span><span class="co">#&gt; mu  0.014 0.000</span></span>
<span><span class="co">#&gt; var 0.000 0.029</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.019 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.052</span></span></code></pre></div>
They are all pretty close. Let’s generate the coverage probability for
the Bootstrapped CIs:
<details><summary>
Click to show/hide R code
</summary><div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">20</span>, <span class="fl">140</span>, <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">coverage_prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">count1</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="va">count2</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">fit.boot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_boot.html">mle_boot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/boot.html" class="external-link">boot</a></span><span class="op">(</span></span>
<span>            data <span class="op">=</span> <span class="va">d</span>,</span>
<span>            statistic <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">ind</span><span class="op">)</span> <span class="op">{</span></span>
<span>                <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="fu">fit_normal</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">ind</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>            <span class="op">}</span>,</span>
<span>            R <span class="op">=</span> <span class="fl">250</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">fit.boot</span><span class="op">)</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">mu</span> <span class="op">&amp;&amp;</span> <span class="va">mu</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count1</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">var</span> <span class="op">&amp;&amp;</span> <span class="va">var</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count2</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="co">#cat("n = ", n, ", coverage = ", coverage_prob[j, ], "\n")</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
</details><p><img src="reference/figures/README-cov-plot-boot-1.png" width="100%"></p>
</div>
<div class="section level2">
<h2 id="prediction-intervals">Prediction intervals<a class="anchor" aria-label="anchor" href="#prediction-intervals"></a>
</h2>
<p>Frequently, we are actually interested in predicting the outcome of
the random variable (or vector) that we are estimating the parameters
of.</p>
<p>We observed a sample <span class="math inline">\(\mathcal{D} =
\{X_i\}_{i=1}^n\)</span> where <span class="math inline">\(X_i \sim
\operatorname{Normal}(\mu, \sigma)\)</span>, <span class="math inline">\(\theta = (\mu, \sigma)\)</span> not known. To
estimate <span class="math inline">\(\theta\)</span>, we solved the MLE
which, asymptotically, is normally distributed with a mean <span class="math inline">\(\theta\)</span> and a variance-covariance given by
the inverse of the FIM (or, using the Bootstrap, by estimating the
covariance of the sampling distribution of the Bootstrapped MLEs).</p>
<p>We wish to model the uncertainty of a new observation, <span class="math inline">\(\hat{X}_{n+1}|\mathcal{D}\)</span>. We do so by
considering both the uncertainty inherent to the Normal distribution and
the uncertainty of our estimate <span class="math inline">\(\hat\theta\)</span>. In particular, we let <span class="math inline">\(\hat{X}_{n+1}|\hat\theta \sim
\operatorname{Normal}(\hat\theta)\)</span> and <span class="math inline">\(\hat\theta \sim
\operatorname{MVN}(\theta,I^{-1}(\theta)/n)\)</span>. Then, the joint
distribution of <span class="math inline">\(\hat{X}_{n+1}\)</span> and
<span class="math inline">\(\hat\theta\)</span> has the pdf given by
<span class="math display">\[
    f(t,\theta) = f_{\hat{X}|\hat\theta}(x|\theta)
f_{\hat\theta}(\theta),
\]</span> and thus to find <span class="math inline">\(f(t)\)</span>, we
marginalize over <span class="math inline">\(\theta\)</span>, obtaining
<span class="math display">\[
    f(x) = \int_{R^2} f_{\hat{X}_{n+1},\hat\theta}(x,\theta) d\theta.
\]</span></p>
<p>Given the information in the sample, the uncertainty in the new
observation is characterized by the distribution <span class="math display">\[
    \hat{X}_{n+1} \sim f(x).
\]</span></p>
<p>It has greater variance than <span class="math inline">\(X_{n+1}|\hat\theta\)</span> because, as stated
earlier, we do not know <span class="math inline">\(\theta\)</span>, we
only have an uncertain estimate <span class="math inline">\(\hat\theta\)</span>.</p>
<p>In <code>pred</code>, we compute the predictive interval (PI) of the
distribution of <span class="math inline">\(\hat{X}_{n+1}\)</span> using
Monte Carlo integration, i.e., sum over a large number of draws from the
joint distribution of <span class="math inline">\(\hat{X}_{n+1}\)</span>
and <span class="math inline">\(\hat\theta\)</span> and then compute the
empirical quantiles.</p>
<p>The function <code>pred</code> takes as arguments <code>x</code>, in
this case an <code>mle</code> object, and a sampler for the distribution
of the random variable of interest, in this case <code>rnorm</code> (the
sampler for the normal distribution). The sampler must be compatible
with the parameter value of <code>x</code> (i.e.,
<code>params(x)</code>). Here is how we compute the PI for <span class="math inline">\(\hat{X}_{n+1}\)</span>:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">samp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">par</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/pred.html">pred</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta.hat</span>, samp <span class="op">=</span> <span class="va">samp</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mean lower upper</span></span>
<span><span class="co">#&gt; [1,] 0.803 -1.47  3.09</span></span></code></pre></div>
<p>How does this compare to <span class="math inline">\(X_{n+1}|\hat\theta\)</span>?</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">par</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="va">mu.hat</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">var.hat</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mu.hat</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.025</span>,<span class="fl">.975</span><span class="op">)</span>, mean <span class="op">=</span> <span class="va">mu.hat</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var.hat</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;     mu               </span></span>
<span><span class="co">#&gt;  0.809 -1.464  3.083</span></span></code></pre></div>
<p>We see that the 95% quantile interval for <span class="math inline">\(X_{n+1}|\hat\theta\)</span> is a bit smaller than
<span class="math inline">\(\hat{X}_{n+1}\)</span>, which is what we
expected. Of course, for sufficiently large sample sizes, they will
converge to the same quantiles.</p>
</div>
<div class="section level2">
<h2 id="weighted-mle-a-weighted-sum-of-maximum-likelihood-estimators">Weighted MLE: a weighted sum of maximum likelihood estimators<a class="anchor" aria-label="anchor" href="#weighted-mle-a-weighted-sum-of-maximum-likelihood-estimators"></a>
</h2>
<p>Since the variance-covariance of an MLE is inversely proportional to
the FIM that the MLE is defined with respect to, we can combine multiple
MLEs of <span class="math inline">\(\theta\)</span>, each of which may
be defined with respect to a different kind of sample, to arrive at the
MLE that incorporates the Fisher information in all of those
samples.</p>
<p>Consider <span class="math inline">\(k\)</span> mutually independent
MLEs of parameter <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat\theta_1,\ldots,\hat\theta_k\)</span>, where
<span class="math inline">\(\hat\theta_j \sim
N(\theta,I_j^{-1}(\theta))\)</span>. Then, the sampling MLE of <span class="math inline">\(\theta\)</span> that incorporates all of the data
in <span class="math inline">\(\hat\theta_1,\ldots,\hat\theta_k\)</span>
is given by the inverse-variance weighted mean, <span class="math display">\[
    \hat\theta_w = \left(\sum_{j=1}^k I_j(\theta)\right)^{-1}
\left(\sum_{j=1}^k I_j(\theta) \hat\theta_j\right),
\]</span> which, asymptotically, has an expected value of <span class="math inline">\(\theta\)</span> and a variance-covariance of <span class="math inline">\(\left(\sum_{j=1}^k
I_j(\theta)\right)^{-1}\)</span>.</p>
<p>To evaluate the performance of the weighted MLE, we generate a sample
of <span class="math inline">\(N=1000\)</span> observations from <span class="math inline">\(\mathcal{N}(\theta)\)</span> and compute the MLE
for the observed sample, denoted by <span class="math inline">\(\hat\theta\)</span>.</p>
<p>We then divide the observed sample into <span class="math inline">\(r=5\)</span> sub-samples, each of size <span class="math inline">\(N/r=100\)</span>, and compute the MLE for each
sub-sampled, denoted by <span class="math inline">\(\theta^{(1)},\ldots,\theta^{(r)}\)</span>.</p>
<p>Finally, we do a weighted combination these MLEs to form the weighted
MLE, denoted by <span class="math inline">\(\theta_w\)</span>:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span><span class="va">samp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">N</span>, mean <span class="op">=</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">samp.sub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="va">samp</span>, nrow <span class="op">=</span> <span class="va">r</span><span class="op">)</span></span>
<span><span class="va">mles.sub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>length <span class="op">=</span> <span class="va">r</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">r</span><span class="op">)</span></span>
<span>    <span class="va">mles.sub</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">samp.sub</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mle.wt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_weighted.html">mle_weighted</a></span><span class="op">(</span><span class="va">mles.sub</span><span class="op">)</span></span>
<span><span class="va">mle</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">samp</span><span class="op">)</span></span></code></pre></div>
<p>We show the results in the following R code. First, we show the
weighted MLE and its MSE:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">mle.wt</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.892 0.624</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">mle.wt</span><span class="op">)</span></span>
<span><span class="co">#&gt;                    [,1]               [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0071944135025693 0.0000000000000243</span></span>
<span><span class="co">#&gt; [2,] 0.0000000000000243 0.0089815176185350</span></span></code></pre></div>
<p>The MLE for the total sample and its MSE is:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">mle</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 0.956 0.860</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">mle</span><span class="op">)</span></span>
<span><span class="co">#&gt;                    [,1]               [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0085950404213935 0.0000000000000141</span></span>
<span><span class="co">#&gt; [2,] 0.0000000000000141 0.0147749439691487</span></span></code></pre></div>
<p>Unfortuantely, <span class="math inline">\(\hat\theta\)</span> is a
much better estimator of <span class="math inline">\(\theta\)</span>
than <span class="math inline">\(\hat\theta_w\)</span>. According to
theory, they should be identical, but in practice, there may be issues
like numerical instability that cause the weighted MLE to perform
poorly.</p>
<p>We are in fact using numerical differentiation to compute the FIM,
which may be a source of error. We can try to improve the accuracy of
the FIM by using a more accurate method of computing the FIM, such as an
analytical solution or a more accurate numerical approximation.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Alexander Towell.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
