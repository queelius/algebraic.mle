<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Statistics and characteristics of the MLE • algebraic.mle</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Statistics and characteristics of the MLE">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">algebraic.mle</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/dgp.html">Dynamic failure rate model</a></li>
    <li><a class="dropdown-item" href="../articles/fitting-common-dist.html">Fitting Common Distributions to a DGP</a></li>
    <li><a class="dropdown-item" href="../articles/statistics.html">Statistics and characteristics of the MLE</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/queelius/algebraic.mle/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Statistics and characteristics of the MLE</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/queelius/algebraic.mle/blob/master/vignettes/statistics.Rmd" class="external-link"><code>vignettes/statistics.Rmd</code></a></small>
      <div class="d-none name"><code>statistics.Rmd</code></div>
    </div>

    
    
<!-- badges: start -->
<p><a href="https://www.gnu.org/licenses/gpl-3.0" class="external-link"><img src="https://img.shields.io/badge/license-GPL--3-blue.svg" alt="GPL-3 License"></a> <!-- badges: end --></p>
<p><code>algebraic.mle</code> is an R package that provides an algebra
over Maximum Likelihood Estimators (MLEs). These estimators possess many
desirable, well-defined statistical properties which the package helps
you manipulate and utilize.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>The R package <code>algebraic.mle</code> can be installed from GitHub
by using the devtools package in R:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"queelius/algebraic.mle"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="normal-distribution">Normal distribution<a class="anchor" aria-label="anchor" href="#normal-distribution"></a>
</h2>
<p>We are going to the classic Normal distribution to demonstrate how to
use <code>algebraic.mle</code>. We are using it for a few reasons:</p>
<ol style="list-style-type: decimal">
<li>It’s well-understood, so we can compare our results to the known
results.</li>
<li>It’s a very common distribution, so it’s useful to have a good
understanding of its properties.</li>
<li>The MLE is multivariate, so we can demonstrate how to use
<code>algebraic.mle</code> for multivariate distributions.</li>
</ol>
<p>So, first, we define a simple MLE solver for the normal
distribution.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_normal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">sigma</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">data</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">loglik</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>        <span class="op">-</span><span class="va">n</span> <span class="op">/</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span></span>
<span>            <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">data</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">+</span> <span class="va">n</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">par.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span>mu <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, var <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html" class="external-link">sigma</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">H</span> <span class="op">&lt;-</span> <span class="fu">numDeriv</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/hessian.html" class="external-link">hessian</a></span><span class="op">(</span>func <span class="op">=</span> <span class="va">loglik</span>, x <span class="op">=</span> <span class="va">par.hat</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></span>
<span>    <span class="fu">algebraic.mle</span><span class="fu">::</span><span class="fu"><a href="../reference/mle.html">mle</a></span><span class="op">(</span></span>
<span>        theta.hat <span class="op">=</span> <span class="va">par.hat</span>,</span>
<span>        loglike <span class="op">=</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">par.hat</span>, <span class="va">data</span><span class="op">)</span>,</span>
<span>        score <span class="op">=</span> <span class="fu">numDeriv</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/grad.html" class="external-link">grad</a></span><span class="op">(</span>func <span class="op">=</span> <span class="va">loglik</span>, x <span class="op">=</span> <span class="va">par.hat</span>, data <span class="op">=</span> <span class="va">data</span><span class="op">)</span>,</span>
<span>        sigma <span class="op">=</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/ginv.html" class="external-link">ginv</a></span><span class="op">(</span><span class="op">-</span><span class="va">H</span><span class="op">)</span>,</span>
<span>        info <span class="op">=</span> <span class="op">-</span><span class="va">H</span>,</span>
<span>        obs <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>        nobs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>,</span>
<span>        superclasses <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mle_normal"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>As you can see, we return an <code>mle</code> object, and then we
give it a sub-class <code>mle_normal</code> (it is also a subclass of
<code>mle</code> and <code>algebraic.dist</code>’s <code>dist</code>) so
we can specialize some of the methods for MLE of the normal
distribution, e.g., <code>bias.mle_normal</code> which we show
later.</p>
</div>
<div class="section level2">
<h2 id="monte-carlo-mc-simulation-of-the-sampling-distribution-of-the-mle">Monte-carlo (MC) simulation of the sampling distribution of the
MLE<a class="anchor" aria-label="anchor" href="#monte-carlo-mc-simulation-of-the-sampling-distribution-of-the-mle"></a>
</h2>
<p>Let’s define <code>theta_samp_mc</code>, which stands for the Monte
Carlo simulation of the sampling distribution of the MLE. It takes a
sample size <code>n</code>, a true parameter value <code>theta</code>,
and a number of simulations <code>B</code> to run. It returns a matrix
with <code>B</code> rows and two columns, the first column is the MLE of
the mean and the second column is the MLE of the variance.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta_samp_mc</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">theta</span>, <span class="va">B</span> <span class="op">=</span> <span class="fl">10000</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">mu</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>    <span class="va">var</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>    <span class="va">mles</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="va">B</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">B</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">mles</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="fu">fit_normal</span><span class="op">(</span><span class="va">d</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">mles</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"var"</span><span class="op">)</span></span>
<span>    <span class="va">mles</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set up the parameters of a simulation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">913254</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">70</span></span>
<span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">var</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">B</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mu</span>, <span class="va">var</span><span class="op">)</span></span>
<span><span class="va">mles</span> <span class="op">&lt;-</span> <span class="fu">theta_samp_mc</span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, theta <span class="op">=</span> <span class="va">theta</span>, B <span class="op">=</span> <span class="va">B</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">mles</span><span class="op">)</span></span>
<span><span class="co">#&gt;         mu   var</span></span>
<span><span class="co">#&gt; [1,] 0.891 0.728</span></span>
<span><span class="co">#&gt; [2,] 1.033 1.102</span></span>
<span><span class="co">#&gt; [3,] 1.105 1.094</span></span>
<span><span class="co">#&gt; [4,] 1.146 0.832</span></span>
<span><span class="co">#&gt; [5,] 0.916 0.895</span></span>
<span><span class="co">#&gt; [6,] 0.986 0.904</span></span></code></pre></div>
<p>The matrix <code>mles</code> is a sample of MLEs from the sampling
distribution of the MLE. It is an <em>empirical distribution</em> of the
MLE
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">(\mu, \sigma^2)'</annotation></semantics></math>
from samples of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X_i \sim N(\mu, \sigma^2)</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">i=1,\ldots,n</annotation></semantics></math>.</p>
<p>This particular example is Monte Carlo simulation of the sampling
distribution, since we are simulating the sampling distribution by
repeatedly sampling from the population distribution and computing the
MLE for each sample.</p>
<blockquote>
<p>In bootstrap, we would <em>resample</em> from the sample, not the
population, but with a large enough sample, the two will produce nearly
identical results. See the bootstrap section for more details, where
we’ll compare the two.</p>
</blockquote>
<p>For a sufficiently large number of simulations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>,
the empirical sampling distribution should be very close to the true
sampling distribution. We can plot the empirical sampling distribution
of the MLEs using the <code>plot</code> function on the
<code>mles</code> matrix.</p>
<div class="figure" style="text-align: center">
<img src="statistics_files/figure-html/unnamed-chunk-3-1.png" class="r-plt" alt="Sampling distribution of the MLEs." width="384"><p class="caption">
Sampling distribution of the MLEs.
</p>
</div>
<p>In <code>algebraic.dist</code>, we can use
<code>empirical_dist</code> to represent an empirical sampling
distribution by giving it the sample of MLEs previously generated:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta.mc</span> <span class="op">&lt;-</span> <span class="fu">algebraic.dist</span><span class="fu">::</span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/empirical_dist.html" class="external-link">empirical_dist</a></span><span class="op">(</span><span class="va">mles</span><span class="op">)</span></span></code></pre></div>
<p>In general, for any MLE and assuming the the regularity conditions
hold, the asymptotic sampling distribution of the MLE is normal with
mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
and variance-covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>=</mo><msup><mi>I</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\Sigma = I^{-1}(\theta)_n</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>I</mi><annotation encoding="application/x-tex">I</annotation></semantics></math>
is the Fisher information matrix and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is the sample size. However, in general:</p>
<ol style="list-style-type: decimal">
<li>We don’t know when the asymptotic sampling distribution is a good
approximation to the true sampling distribution. In these cases, the
empirical sampling distribution may be used instead.</li>
<li>We may not be confident our implementation of the MLE is correct, in
which case the empirical sampling distribution can be used to check our
implementation.</li>
<li>The regularity conditions may not hold, in which case the asymptotic
sampling distribution may not be known. In these cases, the empirical
sampling distribution may be used instead.</li>
</ol>
<p>With these caveats in mind, we compare some of the statistics of the
empirical sampling distribution of the MLE for the normal distribution
and the asymptotic sampling distribution.</p>
<p>Let’s look at some basic parameters of the sampling distribution of
the MLE for the normal distribution. First, let’s look at the mean:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">mu.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 0.995 0.982</span></span></code></pre></div>
<p>The mean looks pretty close to the true parameter vector
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>=</mo><mn>1</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
    \theta = (\mu = 1, \sigma^2 = 1)'.
</annotation></semantics></math></p>
<p>We can actually compute any parameter, since <code>theta.mc</code>,
models the concept of a distribution. In particular, it models a
distribution in <code>algebraic.dist</code>, and thus the API exposed by
<code>algebraic.dist</code> is available to us. For instance, we can
compute various parameters of the sampling distribution of the MLE using
the <code>expectation</code> function:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># should sum to 1</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co"># mean</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 0.995 0.982</span></span>
<span><span class="co"># variance of (mu, var)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt;     mu    var </span></span>
<span><span class="co">#&gt; 0.0136 0.0282</span></span>
<span><span class="co"># kurtosis of (mu, var) </span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span><span class="op">^</span><span class="fl">4</span><span class="op">)</span> <span class="op">/</span></span>
<span>    <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 2.77 3.21</span></span>
<span><span class="co"># skewness of mu and var -- should be (0, 0)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">)</span> <span class="op">/</span> <span class="va">theta</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;        mu       var </span></span>
<span><span class="co">#&gt; 0.0000624 0.0016654</span></span>
<span><span class="co"># covariance of (mu, var) -- should be around 0</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.mc</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0000799</span></span></code></pre></div>
<p>We could use the mean and variance-covariance matrix to parameterize
a multivariate normal distribution (MVN), for instance, but we don’t do
that here.</p>
</div>
<div class="section level2">
<h2 id="bias">Bias<a class="anchor" aria-label="anchor" href="#bias"></a>
</h2>
<p>Bias is a measure of the systematic error of an estimator; it
measures how far its average value is from the true value being
estimated. Formally, it is defined as the difference between the
expected value of the estimator and the true value of the parameter,
i.e.,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>Bias</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>E</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>θ</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
\operatorname{Bias}(\hat\theta) = E_{\hat\theta}(\hat\theta) - \theta,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>E</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><annotation encoding="application/x-tex">E_{\hat\theta}</annotation></semantics></math>
denotes the expectation operator with respect to the sampling
distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>.
(Normally, we drop the subscript in the expectation operator and write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math>
instead of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>E</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><annotation encoding="application/x-tex">E_{\hat\theta}</annotation></semantics></math>
unless it’s not clear from context which expectation operator we are
using.)</p>
<p>When the bias is zero, the estimator is <em>unbiased</em>, otherwise
it is <em>biased</em>. Analytically, the asymptotic bias of the MLE for
the parameters of the normal distribution is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>Bias</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mo>−</mo><mfrac><msup><mi>σ</mi><mn>2</mn></msup><mi>n</mi></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
    \operatorname{Bias}(\hat\theta) = \left(\begin{array}{c}
        0 \\
        -\frac{\sigma^2}{n}
    \end{array}\right).
</annotation></semantics></math> Plugging in the true value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma^2 = 1</annotation></semantics></math>
and the sample size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>70</mn></mrow><annotation encoding="application/x-tex">n = 70</annotation></semantics></math>,
we get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0.014</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0, 0.014)</annotation></semantics></math>.
We may also provide an appropriate implementation of the
<code>bias</code> method in <code>algebraic.mle</code> for
<code>mle_normal</code> (which is what we called our the object that we
returned from <code>fit_normal</code>):</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bias.mle_normal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">par</span> <span class="op">=</span> <span class="cn">NULL</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">par</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">par</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span>mu <span class="op">=</span> <span class="fl">0</span>, var <span class="op">=</span> <span class="op">-</span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/nobs.html" class="external-link">nobs</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Now, let’s compute the bias using this function, and the estimate of
the bias provided by the <code>bias.mle_emp</code>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># first, we sample some data from the true distribution</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># now we fit it to the normal distribution</span></span>
<span><span class="va">theta.hat</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># now we compute the bias, first using the asymptotic theory</span></span>
<span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.hat</span>, <span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt;      mu     var </span></span>
<span><span class="co">#&gt;  0.0000 -0.0143</span></span>
<span><span class="co"># now using the empirical sampling distribution</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span> <span class="co"># mean(theta.mc) - theta</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00507 -0.01768</span></span></code></pre></div>
<p>The asymptotic bias and the empirical bias are pretty close. Let’s
see how the bias of the variance changes as the sample size
increases.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">500</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">bias_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">vars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span>
<span>        <span class="va">vars</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">bias_var</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">vars</span><span class="op">)</span> <span class="op">-</span> <span class="va">var</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p><img src="statistics_files/figure-html/bias-plot-1.png" class="r-plt" alt="" width="576"></p>
<div class="section level3">
<h3 id="variance-covariance-matrix">Variance-covariance matrix<a class="anchor" aria-label="anchor" href="#variance-covariance-matrix"></a>
</h3>
<p>The variance-covariance matrix is one of the more important
statistical measures of an estimator of a parameter vector. It
quantities both the variability of the individual parameter estimates
and how they co-vary with each other.</p>
<p>The variance-covariance matrix of a parameter vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>p</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">\theta = (\theta_1, \ldots, \theta_p)'</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
matrix defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>Var</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>E</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><mspace width="-0.167em"></mspace><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><msub><mi>E</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><msub><mi>E</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\operatorname{Var}(\hat\theta) = E_{\hat\theta}\!\bigl[(\hat\theta - E_{\hat\theta}(\hat\theta))
    (\hat\theta - E_{\hat\theta}(\hat\theta))'\bigr].
</annotation></semantics></math></p>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i, j)</annotation></semantics></math>th
element of the variance-covariance matrix is the covariance between the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>th
elements of the parameter vector, respectively
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\theta_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\theta_j</annotation></semantics></math>.
Thus, the diagonal elements of the variance-covariance matrix are the
variances of the individual parameter estimates, and the off-diagonal
elements are the covariances between the parameter estimates.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span>, digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.011 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.017</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span>, digits<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;        mu   var</span></span>
<span><span class="co">#&gt; mu  0.014 0.000</span></span>
<span><span class="co">#&gt; var 0.000 0.028</span></span></code></pre></div>
<p>They look reasonably close, suggesting at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">n = 500</annotation></semantics></math>,
the asymptotic sampling distribution is a good approximation to the
“true” sampling distribution of the MLE.</p>
</div>
<div class="section level3">
<h3 id="confidence-intervals">Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"></a>
</h3>
<p>We can compute the CI of a parameter using the <code>confint</code>
function:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="co">#&gt;      2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  0.724  1.14</span></span>
<span><span class="co">#&gt; var 0.522  1.04</span></span></code></pre></div>
<p>A very important measure of the accuracy of an estimator is its
coverage probability, which is the probability that the confidence
interval for the parameter estimate contains the true value of the
parameter. If the coverage probability for an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>%</mi></mrow><annotation encoding="application/x-tex">(1-\alpha) \%</annotation></semantics></math>-confidence
interval is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">1-\alpha</annotation></semantics></math>,
then the confidence interval is said to be <em>well-calibrated</em>. If
the coverage probability is less than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">1-\alpha</annotation></semantics></math>,
then the confidence interval is said to be <em>conservative</em>; if the
coverage probability is greater than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">1-\alpha</annotation></semantics></math>,
then the confidence interval is said to be
<em>anti-conservative</em>.</p>
<p>We can estimate it by simulating a large number of samples from the
population distribution and computing the proportion of times the
confidence interval contains the true value of the parameter. We can do
this for both the mean and variance of the normal distribution.</p>
<details><summary>
Click to show/hide R code
</summary><div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">200</span>, <span class="fl">50</span><span class="op">)</span>, <span class="fl">300</span>, <span class="fl">600</span>, <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">coverage_prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">count1</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="va">count2</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span>
<span>        <span class="va">ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">mu</span> <span class="op">&amp;&amp;</span> <span class="va">mu</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count1</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">var</span> <span class="op">&amp;&amp;</span> <span class="va">var</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count2</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
</details><p><img src="statistics_files/figure-html/cov-plot-1.png" class="r-plt" alt="" width="480"></p>
<p>We see that the coverage probability is close to the nominal coverage
probability, and converges to it as the sample size increases. This
suggests that the confidence intervals are well-calibrated.</p>
</div>
</div>
<div class="section level2">
<h2 id="mean-squared-error-matrix">Mean squared error matrix<a class="anchor" aria-label="anchor" href="#mean-squared-error-matrix"></a>
</h2>
<p>The mean squared error (MSE) of an estimator of a parameter vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>MSE</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>E</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
\operatorname{MSE}(\hat\theta) = E\bigl[(\hat\theta - \theta)(\hat\theta - \theta)'\bigr],
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">\hat\theta - \theta</annotation></semantics></math>
is a column vector of differences between the estimator and the true
parameter and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">(\hat\theta - \theta)'</annotation></semantics></math>
is a row vector of the same differences, and we are performing a
standard matrix multiplication between the two vectors. The MSE is a
measure of the average squared error of the estimator. It is a function
of the true parameter value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.</p>
<p>This MSE is a <em>matrix</em>. It is very similar to the
variance-covariance matrix, which is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>Var</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>E</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
\operatorname{Var}(\hat\theta) = E\bigl[(\hat\theta - E(\hat\theta))
    (\hat\theta - E(\hat\theta))'\bigr],
</annotation></semantics></math> where we replace the true paramater
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
with the expected value of the estimator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>.
If the estimator is unbiased, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">E(\hat\theta) = \theta</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>Var</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>MSE</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Var}(\hat\theta) = \operatorname{MSE}(\hat\theta)</annotation></semantics></math>.</p>
<p>We not only need to consider the estimation error for each parameter
individually, but also how these errors might relate to each other. For
instance, it could be the case that when we overestimate one parameter,
we tend to underestimate another. This kind of relationship between
errors in estimating different parameters can be captured by the
off-diagonal elements of the MSE matrix, which represent the covariances
between errors.</p>
<p>The diagonal elements of the MSE represent the MSE of the individual
parameter estimators, e.g., the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
diagonal element represents
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>MSE</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{MSE}(\hat\theta_j)</annotation></semantics></math>.</p>
<p>The <em>trace</em> of the MSE, the sum of the diagonal elements,
represents the total MSE across all parameters. As a single summary
statistic, it may be useful for comparing different estimators.</p>
<p>The MSE can be decomposed into two parts:</p>
<ol style="list-style-type: decimal">
<li>The <em>bias</em>, which is the difference between the expected
value of the estimator and the true parameter value, and</li>
<li>The <em>variance</em>, which is the variance of the estimator.</li>
</ol>
<p>The MSE is then computed as the sum of the bias outer product and the
variance-covariance matrix:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>MSE</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>Bias</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>Bias</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>+</mo><mo>Var</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
\operatorname{MSE}(\hat\theta) = \operatorname{Bias}(\hat\theta)\operatorname{Bias}(\hat\theta)'
    + \operatorname{Var}(\hat\theta).
</annotation></semantics></math></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mse.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">theta.hat</span>, <span class="va">theta</span><span class="op">)</span></span>
<span><span class="va">mse.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>,</span>
<span>    <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span><span class="op">)</span>, nrow <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">mse.hat</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;         mu   var</span></span>
<span><span class="co">#&gt; [1,] 0.011 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.018</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">mse.mc</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.014 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.029</span></span></code></pre></div>
<p>It’s hard to distinguish the MSE matrices from the
variance-covariance matrices reported previously, which is not
surprising, since the bias is relatively small and so the MSE is
dominated by the variance.</p>
<p>Let’s take a closer look at the variance and MSE of the mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>μ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\mu</annotation></semantics></math>:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># temporarily show more digits in the numbers/outputs for this code block</span></span>
<span><span class="va">op</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">12</span><span class="op">)</span></span>
<span><span class="co"># mse(mu)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.013651619343</span></span>
<span><span class="co"># variance(mu)</span></span>
<span><span class="op">(</span><span class="va">mu.var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.013625962941</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span></span>
<span><span class="co"># mse = bias^2 + variance</span></span>
<span><span class="va">b</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">mu.var</span></span>
<span><span class="co">#&gt; [1] 0.013651619343</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span><span class="va">op</span><span class="op">)</span></span></code></pre></div>
<p>They are very close, since the bias is so small.</p>
<p>We should take the MSE from the Monte Carlo simulation as a sort of
“true” MSE, since it is computed from the empirical sampling
distribution of the MLE. We expect that as the sample size increases,
the asymptotic MSE (<code>mse.hat</code>) and the MC MSE
(<code>mse.mc</code>) will converge to the same value. In fact, let’s
run a little experiment to show this:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">200</span>, <span class="fl">25</span><span class="op">)</span></span>
<span><span class="va">mses.mc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mses.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">mses.hat.hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">theta.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/empirical_dist.html" class="external-link">empirical_dist</a></span><span class="op">(</span><span class="fu">theta_samp_mc</span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, theta <span class="op">=</span> <span class="va">theta</span>, B <span class="op">=</span> <span class="va">B</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="co"># Use mean() on the sampled distribution for MSE calculation</span></span>
<span>    <span class="va">samples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/obs.html" class="external-link">obs</a></span><span class="op">(</span><span class="va">theta.n</span><span class="op">)</span></span>
<span>    <span class="va">mse.mu.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">samples</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="va">mse.var.n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">samples</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">var</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span>    <span class="va">mses.mc</span><span class="op">[</span><span class="va">j</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mse.mu.n</span>, <span class="va">mse.var.n</span><span class="op">)</span></span>
<span>    <span class="va">mses.hat</span><span class="op">[</span><span class="va">j</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">fit</span>, <span class="va">theta</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">mses.hat.hat</span><span class="op">[</span><span class="va">j</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="../reference/mse.html">mse</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p><img src="statistics_files/figure-html/mse-plots-1.png" class="r-plt" alt="" width="576"><img src="statistics_files/figure-html/mse-plots-2.png" class="r-plt" alt="" width="576"></p>
<p>These plots demonstrate that the asymptotic MSE is a good
approximation to the “true” MSE, which is the MSE computed from the
empirical sampling distribution of the MLE.</p>
<p>It’s difficult to distinguish the estimated asymptotic MSE, where the
true parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is not known, from the asymptotic MSE, where the true parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is known. This is because the bias is so small, and so the MSE is
dominated by the variance.</p>
</div>
<div class="section level2">
<h2 id="bootstrap-of-the-sampling-distribution-of-the-mle">Bootstrap of the sampling distribution of the MLE<a class="anchor" aria-label="anchor" href="#bootstrap-of-the-sampling-distribution-of-the-mle"></a>
</h2>
<p>Normally, we don’t know the true data generating process (DGP) of the
data we observe. We only have a sample of data, and we want to use that
sample to estimate the parameters of some model that hopefully provides
a good fit to the DGP using maximum likelihood estimation.</p>
<p>Earlier, we simulated a sample of data from a normal distribution
with mean 1 and variance 1 and then used MLE on each sample to generate
an empirical sampling distribution of the MLE. This is called <em>Monte
Carlo simulation</em>.</p>
<p>However, we can also use the sample of data we have to generate an
empirical sampling distribution of the MLE. This is called
<em>bootstrap</em>. The idea is that the sample we have is a sample from
the true DGP, and we can use that sample to generate new samples
(resample) and fit an MLE to each of these to generate an Bootstrapped
empirical sampling distribution of the MLE.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate a sample of n observations from a normal with mean 1 and variance 2.</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">boot</span><span class="op">)</span></span>
<span><span class="va">theta.boot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_boot.html">mle_boot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/boot.html" class="external-link">boot</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">data</span>,</span>
<span>    statistic <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">ind</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="fu">fit_normal</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">ind</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span>,</span>
<span>    R <span class="op">=</span> <span class="va">B</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Let’s compute some statistics:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mu  var </span></span>
<span><span class="co">#&gt; 1.09 1.01</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span></span>
<span><span class="co">#&gt;      2.5% 97.5%</span></span>
<span><span class="co">#&gt; mu  0.952  1.22</span></span>
<span><span class="co">#&gt; var 0.825  1.20</span></span></code></pre></div>
<p>Let’s use these Bootstrapped MLEs to generate an aproximation of the
empirical sampling distribution:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/empirical_dist.html" class="external-link">empirical_dist</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">$</span><span class="va">t</span><span class="op">)</span></span></code></pre></div>
<p>As before, let’s do some basic expectations of the Bootstrapped
sampling distribution of the MLE and compare to the previous
results:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># should sum to 1</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co"># mean</span></span>
<span><span class="op">(</span><span class="va">mu.b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">theta.b</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.08 1.01</span></span>
<span><span class="co"># variance of (mu, var)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.00471 0.00937</span></span>
<span><span class="co"># kurtosis of (mu, var) </span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span><span class="op">^</span><span class="fl">4</span><span class="op">)</span> <span class="op">/</span></span>
<span>    <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="co">#&gt; [1] 2.84 2.91</span></span>
<span><span class="co"># skewness of mu and var -- should be (0, 0)</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">)</span> <span class="op">/</span> <span class="va">theta</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0000123 0.0001203</span></span>
<span><span class="co"># covariance of (mu, var) -- should be around 0</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.b</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">mu.b</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.000795</span></span></code></pre></div>
<p>These are not too bad.</p>
<p>Let’s compute the bias and compare it to the previous results:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.boot</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00144 -0.00459</span></span>
<span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/expectation.html" class="external-link">expectation</a></span><span class="op">(</span><span class="va">theta.mc</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mu      var </span></span>
<span><span class="co">#&gt; -0.00507 -0.01768</span></span>
<span><span class="fu"><a href="../reference/bias.html">bias</a></span><span class="op">(</span><span class="va">theta.hat</span>, <span class="va">theta</span><span class="op">)</span></span>
<span><span class="co">#&gt;      mu     var </span></span>
<span><span class="co">#&gt;  0.0000 -0.0143</span></span></code></pre></div>
<p>We see that the <code>bias</code> function for <code>mle_boot</code>
is not too bad. Note that the <code>bias</code> is an expectation w.r.t.
the sampling distribution of the MLE. In general, we can have a better
estimator if we use
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>*</mo></msup><mo>=</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>−</mo><mo>Bias</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
    \hat\theta^* = \hat\theta - \operatorname{Bias}(\hat\theta),
</annotation></semantics></math> assuming the bias estimate is accurate.
In this particular example that transformation makes it worse, which is
fine, the bias of the transformed estimator would be less in theory.
Howevever, in practice, we don’t trust the bias reported by the
Bootstrap, except as evidence that our estimator is biased or not. The
analytic bias, <code>bias.mle_normal</code>, is more accurate, and will
generally produce estimators with less bias (although by the
bias-variance trade-off, it may have more variance).</p>
<p>Let’s compare the variance-covariance matrix of the Bootstrapped
sampling distribution of the MLE to the “true” sampling distribution and
the asymptotic sampling distribution:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.b</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.005 0.001</span></span>
<span><span class="co">#&gt; [2,] 0.001 0.009</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.mc</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;        mu   var</span></span>
<span><span class="co">#&gt; mu  0.014 0.000</span></span>
<span><span class="co">#&gt; var 0.000 0.028</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;       [,1]  [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.011 0.000</span></span>
<span><span class="co">#&gt; [2,] 0.000 0.017</span></span></code></pre></div>
They are all pretty close. Let’s generate the coverage probability for
the Bootstrapped CIs:
<details><summary>
Click to show/hide R code
</summary><div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">ns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">50</span>, <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">coverage_prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="cn">NA</span>, nrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">ns</span><span class="op">)</span>, ncol<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">j</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="va">ns</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">count1</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="va">count2</span> <span class="op">&lt;-</span> <span class="fl">0L</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">mu</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">fit.boot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_boot.html">mle_boot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/boot.html" class="external-link">boot</a></span><span class="op">(</span></span>
<span>            data <span class="op">=</span> <span class="va">d</span>,</span>
<span>            statistic <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">ind</span><span class="op">)</span> <span class="op">{</span></span>
<span>                <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="fu">fit_normal</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">ind</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span>            <span class="op">}</span>,</span>
<span>            R <span class="op">=</span> <span class="fl">250</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="va">ci</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">fit.boot</span><span class="op">)</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">mu</span> <span class="op">&amp;&amp;</span> <span class="va">mu</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count1</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>        <span class="kw">if</span> <span class="op">(</span><span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;=</span> <span class="va">var</span> <span class="op">&amp;&amp;</span> <span class="va">var</span> <span class="op">&lt;=</span> <span class="va">ci</span><span class="op">[</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>            <span class="va">count2</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>        <span class="op">}</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count1</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="va">coverage_prob</span><span class="op">[</span><span class="va">j</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">count2</span> <span class="op">/</span> <span class="va">N</span></span>
<span>    <span class="co">#cat("n = ", n, ", coverage = ", coverage_prob[j, ], "\n")</span></span>
<span>    <span class="va">j</span> <span class="op">&lt;-</span> <span class="va">j</span> <span class="op">+</span> <span class="fl">1</span></span>
<span><span class="op">}</span></span></code></pre></div>
</details><p><img src="statistics_files/figure-html/cov-plot-boot-1.png" class="r-plt" alt="" width="480"></p>
</div>
<div class="section level2">
<h2 id="prediction-intervals">Prediction intervals<a class="anchor" aria-label="anchor" href="#prediction-intervals"></a>
</h2>
<p>Frequently, we are actually interested in predicting the outcome of
the random variable (or vector) that we are estimating the parameters
of.</p>
<p>We observed a sample
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒟</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>X</mi><mi>i</mi></msub><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{D} = \{X_i\}_{i=1}^n</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∼</mo><mo>Normal</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X_i \sim \operatorname{Normal}(\mu, \sigma)</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta = (\mu, \sigma)</annotation></semantics></math>
not known. To estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
we solved the MLE which, asymptotically, is normally distributed with a
mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
and a variance-covariance given by the inverse of the FIM (or, using the
Bootstrap, by estimating the covariance of the sampling distribution of
the Bootstrapped MLEs).</p>
<p>We wish to model the uncertainty of a new observation,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><mi>𝒟</mi></mrow><annotation encoding="application/x-tex">\hat{X}_{n+1}|\mathcal{D}</annotation></semantics></math>.
We do so by considering both the uncertainty inherent to the Normal
distribution and the uncertainty of our estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>.
In particular, we let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>∼</mo><mo>Normal</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{X}_{n+1}|\hat\theta \sim \operatorname{Normal}(\hat\theta)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo>∼</mo><mo>MVN</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><msup><mi>I</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat\theta \sim \operatorname{MVN}(\theta,I^{-1}(\theta)/n)</annotation></semantics></math>.
Then, the joint distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{n+1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>
has the pdf given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>f</mi><mrow><mover><mi>X</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="prefix">|</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>f</mi><mover><mi>θ</mi><mo accent="true">̂</mo></mover></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
    f(t,\theta) = f_{\hat{X}|\hat\theta}(x|\theta) f_{\hat\theta}(\theta),
</annotation></semantics></math> and thus to find
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math>,
we marginalize over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
obtaining
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mo>∫</mo><msup><mi>R</mi><mn>2</mn></msup></msub><msub><mi>f</mi><mrow><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>θ</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
    f(x) = \int_{R^2} f_{\hat{X}_{n+1},\hat\theta}(x,\theta) d\theta.
</annotation></semantics></math></p>
<p>Given the information in the sample, the uncertainty in the new
observation is characterized by the distribution
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
    \hat{X}_{n+1} \sim f(x).
</annotation></semantics></math></p>
<p>It has greater variance than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">X_{n+1}|\hat\theta</annotation></semantics></math>
because, as stated earlier, we do not know
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
we only have an uncertain estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>.</p>
<p>In <code>pred</code>, we compute the predictive interval (PI) of the
distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{n+1}</annotation></semantics></math>
using Monte Carlo integration, i.e., sum over a large number of draws
from the joint distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{n+1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>
and then compute the empirical quantiles.</p>
<p>The function <code>pred</code> takes as arguments <code>x</code>, in
this case an <code>mle</code> object, and a sampler for the distribution
of the random variable of interest, in this case <code>rnorm</code> (the
sampler for the normal distribution). The sampler must be compatible
with the parameter value of <code>x</code> (i.e.,
<code>params(x)</code>). Here is how we compute the PI for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{n+1}</annotation></semantics></math>:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">samp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">par</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="va">n</span>, mean <span class="op">=</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/pred.html">pred</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta.hat</span>, samp <span class="op">=</span> <span class="va">samp</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mean  lower upper</span></span>
<span><span class="co">#&gt; [1,] 0.933 -0.824   2.7</span></span></code></pre></div>
<p>How does this compare to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">X_{n+1}|\hat\theta</annotation></semantics></math>?</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">par</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">theta.hat</span><span class="op">)</span></span>
<span><span class="va">mu.hat</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">var.hat</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">mu.hat</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.025</span>,<span class="fl">.975</span><span class="op">)</span>, mean <span class="op">=</span> <span class="va">mu.hat</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">var.hat</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;     mu               </span></span>
<span><span class="co">#&gt;  0.931 -0.801  2.663</span></span></code></pre></div>
<p>We see that the 95% quantile interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false" form="prefix">|</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">X_{n+1}|\hat\theta</annotation></semantics></math>
is a bit smaller than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>X</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\hat{X}_{n+1}</annotation></semantics></math>,
which is what we expected. Of course, for sufficiently large sample
sizes, they will converge to the same quantiles.</p>
</div>
<div class="section level2">
<h2 id="weighted-mle-a-weighted-sum-of-maximum-likelihood-estimators">Weighted MLE: a weighted sum of maximum likelihood estimators<a class="anchor" aria-label="anchor" href="#weighted-mle-a-weighted-sum-of-maximum-likelihood-estimators"></a>
</h2>
<p>Since the variance-covariance of an MLE is inversely proportional to
the FIM that the MLE is defined with respect to, we can combine multiple
MLEs of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
each of which may be defined with respect to a different kind of sample,
to arrive at the MLE that incorporates the Fisher information in all of
those samples.</p>
<p>Consider
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
mutually independent MLEs of parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\hat\theta_1,\ldots,\hat\theta_k</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><msubsup><mi>I</mi><mi>j</mi><mrow><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat\theta_j \sim N(\theta,I_j^{-1}(\theta))</annotation></semantics></math>.
Then, the sampling MLE of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
that incorporates all of the data in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\hat\theta_1,\ldots,\hat\theta_k</annotation></semantics></math>
is given by the inverse-variance weighted mean,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>w</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>I</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>I</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
    \hat\theta_w = \left(\sum_{j=1}^k I_j(\theta)\right)^{-1} \left(\sum_{j=1}^k I_j(\theta) \hat\theta_j\right),
</annotation></semantics></math> which, asymptotically, has an expected
value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
and a variance-covariance of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mi>I</mi><mi>j</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\left(\sum_{j=1}^k I_j(\theta)\right)^{-1}</annotation></semantics></math>.</p>
<p>To evaluate the performance of the weighted MLE, we generate a sample
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">N=1000</annotation></semantics></math>
observations from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒩</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(\theta)</annotation></semantics></math>
and compute the MLE for the observed sample, denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>.</p>
<p>We then divide the observed sample into
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">r=5</annotation></semantics></math>
sub-samples, each of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>/</mi><mi>r</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N/r=100</annotation></semantics></math>,
and compute the MLE for each sub-sampled, denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mi>…</mi><mo>,</mo><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\theta^{(1)},\ldots,\theta^{(r)}</annotation></semantics></math>.</p>
<p>Finally, we do a weighted combination these MLEs to form the weighted
MLE, denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>θ</mi><mi>w</mi></msub><annotation encoding="application/x-tex">\theta_w</annotation></semantics></math>:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">100</span></span>
<span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span><span class="va">samp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">N</span>, mean <span class="op">=</span> <span class="va">theta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">theta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">samp.sub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="va">samp</span>, nrow <span class="op">=</span> <span class="va">r</span><span class="op">)</span></span>
<span><span class="va">mles.sub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>length <span class="op">=</span> <span class="va">r</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">r</span><span class="op">)</span></span>
<span>    <span class="va">mles.sub</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">samp.sub</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mle.wt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mle_weighted.html">mle_weighted</a></span><span class="op">(</span><span class="va">mles.sub</span><span class="op">)</span></span>
<span><span class="va">mle</span> <span class="op">&lt;-</span> <span class="fu">fit_normal</span><span class="op">(</span><span class="va">samp</span><span class="op">)</span></span></code></pre></div>
<p>We show the results in the following R code. First, we show the
weighted MLE and its MSE:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">mle.wt</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.221 0.778</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">mle.wt</span><span class="op">)</span></span>
<span><span class="co">#&gt;                    [,1]               [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0080147769950214 0.0000000000000257</span></span>
<span><span class="co">#&gt; [2,] 0.0000000000000257 0.0124697701682341</span></span></code></pre></div>
<p>The MLE for the total sample and its MSE is:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://queelius.github.io/algebraic.dist/reference/params.html" class="external-link">params</a></span><span class="op">(</span><span class="va">mle</span><span class="op">)</span></span>
<span><span class="co">#&gt;    mu   var </span></span>
<span><span class="co">#&gt; 1.210 0.864</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">mle</span><span class="op">)</span></span>
<span><span class="co">#&gt;                    [,1]               [,2]</span></span>
<span><span class="co">#&gt; [1,] 0.0086408549278714 0.0000000000000244</span></span>
<span><span class="co">#&gt; [2,] 0.0000000000000244 0.0149328747769700</span></span></code></pre></div>
<p>Unfortuantely,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>
is a much better estimator of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>w</mi></msub><annotation encoding="application/x-tex">\hat\theta_w</annotation></semantics></math>.
According to theory, they should be identical, but in practice, there
may be issues like numerical instability that cause the weighted MLE to
perform poorly.</p>
<p>We are in fact using numerical differentiation to compute the FIM,
which may be a source of error. We can try to improve the accuracy of
the FIM by using a more accurate method of computing the FIM, such as an
analytical solution or a more accurate numerical approximation.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Alexander Towell.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
